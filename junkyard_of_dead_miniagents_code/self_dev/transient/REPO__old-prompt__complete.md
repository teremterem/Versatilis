File list:
```
README.md
examples/conversation.py
examples/llm_example.py
examples/self_dev/docu_agents.py
examples/self_dev/self_dev_common.py
examples/self_dev/self_dev_prompts.py
miniagents/chat_history.py
miniagents/ext/chat_history_md.py
miniagents/ext/console_user_agent.py
miniagents/ext/llm/anthropic.py
miniagents/ext/llm/llm_common.py
miniagents/ext/llm/openai.py
miniagents/messages.py
miniagents/miniagent_typing.py
miniagents/miniagents.py
miniagents/promising/errors.py
miniagents/promising/ext/frozen.py
miniagents/promising/promise_typing.py
miniagents/promising/promising.py
miniagents/promising/sentinels.py
miniagents/promising/sequence.py
miniagents/utils.py
pyproject.toml
tests/test_agents.py
tests/test_frozen.py
tests/test_llm.py
tests/test_message.py
tests/test_message_sequence.py
tests/test_promise.py
tests/test_sequence.py
```



README.md
```
# MiniAgents

⚠️ ATTENTION! THIS README IS WORK IN PROGRESS (autogenerated with `Claude 3 Opus` and `GPT 4o` and not corrected yet) ⚠️

---

Why does this framework exist at all? What is the problem it solves? What is the motivation behind it?
TODO Oleksandr: explain the motivation behind the framework

---

TODO AI: generate pseudo-FAQ

---

MiniAgents is a Python framework for building agent-based systems. It provides a simple and intuitive way to define
agents and their interactions.

MiniAgents is a Python framework designed to facilitate the creation and management of agents that interact with
language models (LLMs) and other services. It provides a structured way to define, call, and manage agents, making it
easier to build complex systems that rely on asynchronous interactions and streaming data.

An asynchronous framework for building LLM-based multi-agent systems in Python, with a focus on immutable messages and
token streaming.

## Features

- **Agent Management**: Define and manage agents using simple decorators.
- **Agent Management**: Easily create, manage, and chain multiple agents.
- Manage chat history and persist messages
- **Chat History**: Manage chat history with support for in-memory and markdown file storage.
- Flexible chat history management, including in-memory and Markdown-based persistence
- **Asynchronous Interaction**: Support for asynchronous interactions with agents.
- **Streaming**: Stream data token by token or message by message.
- Asynchronous and parallel execution of agents
- Define agents as simple Python functions decorated with `@miniagent`
- Agents can interact with each other by sending and receiving messages
- Agents can send and receive messages asynchronously
- Agents can run in parallel and communicate with each other
- Agents can be composed to create more complex agents
- Agents can be chained together to form complex interaction flows
- Promises and async iterators are used extensively to enable non-blocking communication
- Pass messages between agents using `MessageType` objects
- Integrate with OpenAI and Anthropic LLMs using `openai_agent` and `anthropic_agent`
- Extensible architecture allows integration with various LLM providers (OpenAI, Anthropic, etc.)
- **LLM Integration**: Seamlessly integrate with popular LLMs like OpenAI and Anthropic.
- **Message Handling**: Robust message handling with support for nested messages and promises.
- Supports streaming of messages and tokens for efficient processing
- Utilities for working with message sequences (joining, splitting, etc.)
- **Utilities**: A set of utility functions to facilitate common tasks like dialog loops and message joining.
- Utilities for common interaction patterns like dialog loops and agent chaining
- Stream tokens from LLMs piece-by-piece using `StreamedPromise`
- Flatten nested message sequences with `MessageSequence`
- Promises and async iterators used extensively to enable non-blocking execution
- **Immutable Messages**: Ensures that messages are immutable, making the system more predictable and easier to debug.
- Immutable messages for predictable and reproducible agent behavior
- Immutable message passing via `Frozen` pydantic models
- Frozen data structures for immutable agent state and message metadata
- Immutable message and agent state for reproducibility
- Built on top of the `Promising` library for managing asynchronous operations
- Asynchronous promise-based programming model with `Promise` and `StreamedPromise`
- Hooks to persist messages as they are sent/received
- Typing with Pydantic for validation and serialization of messages

## Installation

```bash
pip install miniagents
```

## Usage

Here's a simple example of how to define an agent:

```python
from miniagents import miniagent, InteractionContext


@miniagent
async def my_agent(ctx: InteractionContext):
    async for message in ctx.messages:
        ctx.reply(f"You said: {message}")
```

And here's how to initiate an interaction with the agent:

```python
from miniagents import MiniAgents

async with MiniAgents():
    reply = await my_agent.inquire("Hello!")
    print(reply)  # prints "You said: Hello!"
```

For more advanced usage, check out the [examples](examples/) directory.

## Usage

Here's a simple example of defining agents and having them interact:

```python
from miniagents import miniagent, MiniAgents, InteractionContext


@miniagent
async def agent1(ctx: InteractionContext):
    ctx.reply("Hello from Agent 1!")


@miniagent
async def agent2(ctx: InteractionContext):
    message = await ctx.messages.aresolve_messages()
    ctx.reply(f"Agent 2 received: {message[0].text}")


async def main():
    async with MiniAgents():
        agent2_replies = agent2.inquire(agent1.inquire())
        print(await agent2_replies.aresolve_messages())


asyncio.run(main())
```

This will output:

```
(Message(text='Agent 2 received: Hello from Agent 1!'),)
```

For more advanced usage, including integration with LLMs, see the documentation and examples.

### Basic Example

Here's a basic example of how to create and run a simple agent using MiniAgents:

```python
import asyncio
from miniagents.miniagents import MiniAgents, miniagent, InteractionContext


@miniagent
async def simple_agent(ctx: InteractionContext) -> None:
    print("Agent is running")
    ctx.reply("Hello from the agent!")


async def main() -> None:
    async with MiniAgents():
        await simple_agent.inquire()


if __name__ == "__main__":
    asyncio.run(main())
```

### Define an Agent

You can define an agent using the `@miniagent` decorator. An agent is essentially an asynchronous function that
interacts with a context.

```python
from miniagents.miniagents import miniagent, InteractionContext


@miniagent
async def my_agent(ctx: InteractionContext) -> None:
    ctx.reply("Hello, I am an agent!")
```

### Run an Agent

To run an agent, you need to create an instance of `MiniAgents` and use the `inquire` method to send messages to the
agent.

```python
from miniagents.miniagents import MiniAgents


async def main():
    async with MiniAgents():
        replies = my_agent.inquire()
        async for reply in replies:
            print(await reply)


import asyncio

asyncio.run(main())
```

### Integrate with LLMs

MiniAgents provides built-in support for OpenAI and Anthropic language models. You can create agents for these models
using the provided functions.

```python
from miniagents.ext.llm.openai import openai_agent
from miniagents.messages import Message

openai_agent = openai_agent.fork(model="gpt-3.5-turbo")


async def main():
    async with MiniAgents():
        replies = openai_agent.inquire(
            Message(text="Hello, how are you?", role="user"),
            system="You are a helpful assistant.",
            max_tokens=50,
            temperature=0.7,
        )
        async for reply in replies:
            print(await reply)


import asyncio

asyncio.run(main())
```

## Basic Usage

Here's a simple example of using MiniAgents to create a dialog between a user and an AI assistant powered by OpenAI's
GPT-3.5-turbo model:

```python
from miniagents.ext.llm.openai import openai_agent
from miniagents.ext.console_user_agent import console_user_agent
from miniagents.utils import adialog_loop


async def main():
    assistant_agent = openai_agent.fork(model="gpt-3.5-turbo")

    await adialog_loop(console_user_agent, assistant_agent)


asyncio.run(main())
```

This will start an interactive dialog where the user can chat with the AI assistant in the console.

In this example:

1. We take `console_user_agent`, which reads user input from the console and writes back to the console.
2. We create an assistant agent using `openai_agent.fork()`, specifying the OpenAI model to use (e.g., "gpt-3.5-turbo").
3. We start a dialog loop using `adialog_loop()`, passing the user agent and assistant agent as arguments.
4. The dialog loop runs asynchronously within the `MiniAgents` context, allowing the agents to interact and exchange
   messages.

### Basic Example

Here's a simple example of a conversation using the MiniAgents framework:

```python
import logging
from dotenv import load_dotenv
from miniagents.ext.chat_history_md import ChatHistoryMD
from miniagents.ext.console_user_agent import console_user_agent
from miniagents.ext.llm.openai import openai_agent
from miniagents.miniagents import MiniAgents
from miniagents.utils import adialog_loop

load_dotenv()


async def amain() -> None:
    chat_history = ChatHistoryMD("CHAT.md")
    try:
        print()
        await adialog_loop(
            user_agent=console_user_agent.fork(chat_history=chat_history),
            assistant_agent=openai_agent.fork(model="gpt-4o-2024-05-13"),
        )
    except KeyboardInterrupt:
        print()


if __name__ == "__main__":
    logging.basicConfig(level=logging.WARNING)
    MiniAgents().run(amain())
```

### Integrating with OpenAI

To create an agent that interacts with OpenAI, you can use the `openai_agent` function:

```python
from miniagents.ext.llm.openai import openai_agent

# Running the OpenAI agent
mini_agents.run(openai_agent.inquire("Hello, OpenAI!"))
```

### Integrating with Anthropic

Similarly, you can create an agent that interacts with Anthropic:

```python
from miniagents.ext.llm.anthropic import anthropic_agent

# Running the Anthropic agent
mini_agents.run(anthropic_agent.inquire("Hello, Anthropic!"))
```

### Integrating with OpenAI

You can create an agent that interacts with OpenAI's GPT models:

```python
from dotenv import load_dotenv
from miniagents.ext.llm.openai import openai_agent
from miniagents.miniagents import MiniAgents

load_dotenv()

llm_agent = openai_agent.fork(model="gpt-4o-2024-05-13")


async def main() -> None:
    async with MiniAgents():
        reply_sequence = llm_agent.inquire("How are you today?", max_tokens=1000, temperature=0.0)
        async for msg_promise in reply_sequence:
            async for token in msg_promise:
                print(token, end="", flush=True)
            print()


if __name__ == "__main__":
    asyncio.run(main())
```

### Advanced Example

For more advanced usage, you can define multiple agents and manage their interactions:

```python
from miniagents.miniagents import MiniAgents, miniagent, InteractionContext
from miniagents.promising.sentinels import AWAIT
from miniagents.utils import achain_loop


@miniagent
async def user_agent(ctx: InteractionContext) -> None:
    async for msg_promise in ctx.messages:
        async for token in msg_promise:
            print(token, end="", flush=True)
        print()
    ctx.reply(input("USER: "))


@miniagent
async def assistant_agent(ctx: InteractionContext) -> None:
    async for msg_promise in ctx.messages:
        async for token in msg_promise:
            print(token, end="", flush=True)
        print()
    ctx.reply("Hello, how can I assist you?")


async def amain() -> None:
    await achain_loop([user_agent, AWAIT, assistant_agent])


if __name__ == "__main__":
    MiniAgents().run(amain())
```

TODO Oleksandr: explain why AWAIT is used in the example above

### Advanced Example with Multiple Agents

You can create more complex interactions involving multiple agents:

```python
import asyncio
from miniagents.miniagents import MiniAgents, miniagent, InteractionContext


@miniagent
async def agent1(ctx: InteractionContext) -> None:
    print("Agent 1 is running")
    ctx.reply("Message from Agent 1")


@miniagent
async def agent2(ctx: InteractionContext) -> None:
    print("Agent 2 is running")
    ctx.reply("Message from Agent 2")


@miniagent
async def aggregator_agent(ctx: InteractionContext) -> None:
    ctx.reply([agent1.inquire(), agent2.inquire()])


async def main() -> None:
    async with MiniAgents():
        await aggregator_agent.inquire()


if __name__ == "__main__":
    asyncio.run(main())
```

### Message Handling

MiniAgents provides a structured way to handle messages using the `Message` class and its derivatives.

You can create custom message types by subclassing `Message`.

```python
from miniagents.messages import Message


class CustomMessage(Message):
    custom_field: str


message = CustomMessage(text="Hello", custom_field="Custom Value")
print(message.text)  # Output: Hello
print(message.custom_field)  # Output: Custom Value
```

### Handling Messages

MiniAgents provides a structured way to handle messages. You can define different types of messages such
as `UserMessage`, `SystemMessage`, and `AssistantMessage`:

```python
from miniagents.ext.llm.llm_common import UserMessage, SystemMessage, AssistantMessage

user_message = UserMessage(text="Hello!")
system_message = SystemMessage(text="System message")
assistant_message = AssistantMessage(text="Assistant message")
```

TODO Oleksandr: mention that exceptions in agents are treated as messages ?

## Utility Functions

### Joining Messages

You can join multiple messages into a single message using the `join_messages` function:

```python
from miniagents.utils import join_messages


async def main():
    messages = ["Hello", "world"]
    joined_message = join_messages(messages)
    print(await joined_message.aresolve())


miniagents.run(main())
```

### Splitting Messages

You can split a message into multiple messages using the `split_messages` function:

```python
from miniagents.utils import split_messages


async def main():
    message = "Hello\n\nworld"
    split_message = split_messages(message)
    print(await split_message.aresolve_messages())


miniagents.run(main())
```

## Utilities

MiniAgents provides several utility functions to help with common tasks:

- **join_messages**: Join multiple messages into a single message.
- **split_messages**: Split a message into multiple messages based on a delimiter.

Example of joining messages:

```python
from miniagents.utils import join_messages


async def main():
    async with MiniAgents() as context:
        joined_message = join_messages(["Hello", "World"], delimiter=" ")
        print(await joined_message.aresolve())


MiniAgents().run(main())
```

## Documentation

### Modules

- `miniagents`: Core classes and functions.
- `miniagents.ext`: Extensions for integrating with external services and libraries.
- `miniagents.promising`: Classes and functions for handling promises and asynchronous operations.
- `miniagents.utils`: Utility functions for common tasks.

The framework is organized into several modules:

- `miniagents.miniagents`: Core classes for creating and managing agents
- `miniagents.messages`: Classes for representing and handling messages
- `miniagents.promising`: Utilities for managing asynchronous operations using promises
- `miniagents.ext`: Extensions for integrating with external services and utilities
    - `miniagents.ext.chat_history_md`: Chat history management using Markdown files
    - `miniagents.ext.console_user_agent`: User agent for interacting via the console
    - `miniagents.ext.llm`: Integration with language models
        - `miniagents.ext.llm.openai`: OpenAI language model integration
        - `miniagents.ext.llm.anthropic`: Anthropic language model integration

For detailed documentation on each module and class, please refer to the docstrings in the source code.

### Extending MiniAgents

You can extend the functionality of MiniAgents by creating custom agents, message types, and chat history handlers. The
framework is designed to be modular and flexible, allowing you to integrate it with various services and customize its
behavior to fit your needs.

### Core Concepts

#### MiniAgents

`MiniAgents` is the main context manager that handles the lifecycle of agents and promises.

```python
from miniagents import MiniAgents

async with MiniAgents():
# Your code here
```

#### MiniAgent

A `MiniAgent` is a wrapper for an agent function that allows calling the agent.

```python
from miniagents import miniagent


@miniagent
async def my_agent(ctx, **kwargs):
# Agent logic here
```

- `MiniAgents`: The main context manager for running agents
- **MiniAgents**: The main class that manages the lifecycle of agents and their interactions.
- `@miniagent`: Decorator for defining agents
- **MiniAgent**: A wrapper for an agent function that allows calling the agent.
- `MiniAgent` - A wrapper around a Python function that allows it to send and receive messages
- **InteractionContext**: Provides context for the interaction, including the messages and the agent.
- **Message**: Represents a message that can be sent between agents.
- `Message` - Represents a message that can be sent between agents, with optional metadata
- **MessagePromise**: A promise of a message that can be streamed token by token.
- **MessageSequencePromise**: A promise of a sequence of messages that can be streamed message by message.

- `openai_agent`: an OpenAI language model agent
- `anthropic_agent`: an Anthropic language model agent

### Core Classes

- `MiniAgents`: The main context manager for managing agents and their interactions.
- `MiniAgent`: A wrapper for an agent function that allows calling the agent.
- `InteractionContext`: Provides context for an agent's interaction, including the messages and reply streamer.
- `InteractionContext`: Passed to agent functions, provides methods for replying and finishing early

### Message Handling

- `Message`: Represents a message that can be sent between agents.
- `Message`: Represents a message passed between agents
- `MessagePromise`: A promise of a message that can be streamed token by token.
- `MessagePromise`: A promise that resolves to a message
- `MessageSequencePromise`: A promise of a sequence of messages that can be streamed message by message.
- `ChatHistory`: An abstract class for managing chat history.

### Promising

- `Promise`: Represents a promise of a value that will be resolved asynchronously.
- `StreamedPromise`: Represents a promise of a whole value that can be streamed piece by piece.
- `StreamAppender`: Allows appending pieces to a stream that is consumed by a `StreamedPromise`.

### Utilities

- `adialog_loop`: Run a dialog loop between a user agent and assistant agent
- `achain_loop`: Run a loop that chains multiple agents together
- `achain_loop`: Runs a loop of agents, chaining their interactions.
- `join_messages`: Joins multiple messages into a single message using a delimiter.
- `split_messages`: Splits messages based on a delimiter.

## Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.

## License

MiniAgents is released under the [MIT License](LICENSE).

## Things to remember (for the developers of this framework)

- **Different Promise and StreamedPromise resolvers, piece streamers, appenders and what not should always catch
  BaseExceptions and not just Exceptions** when they capture errors to pass those errors as "pieces" in order for
  those errors to be raised at the "consumer side". This is because many of the aforementioned Promising "primitives"
  are often part of mechanisms that involve communications between async tasks via asyncio.Queue objects and just
  interrupting those promises with KeyboardInterrupt which are extended from BaseException instead of letting
  KeyboardInterrupt to go through the queue leads to hanging of those promises (a queue is waiting for END_OF_QUEUE
  sentinel forever but the task that should send it is dead).

---

This README provides an overview of the MiniAgents framework, its features, installation instructions, usage examples,
and information on testing and contributing. For more detailed documentation, please refer to the source code and
comments within the project.

---

Happy coding with MiniAgents! 🚀
```



examples/conversation.py
```python
"""
A simple conversation example using the MiniAgents framework.
"""

import logging

from dotenv import load_dotenv

from miniagents.ext.chat_history_md import ChatHistoryMD
from miniagents.ext.console_user_agent import console_user_agent
from miniagents.ext.llm.openai import openai_agent
from miniagents.miniagents import MiniAgents
from miniagents.utils import adialog_loop

load_dotenv()


async def amain() -> None:
    """
    The main conversation loop.
    """
    chat_history = ChatHistoryMD("CHAT.md")
    try:
        print()
        await adialog_loop(
            user_agent=console_user_agent.fork(chat_history=chat_history),
            assistant_agent=openai_agent.fork(model="gpt-4o-2024-05-13"),
        )
    except KeyboardInterrupt:
        print()


if __name__ == "__main__":
    logging.basicConfig(level=logging.WARNING)
    # logging.getLogger("miniagents.ext.llm").setLevel(logging.DEBUG)

    MiniAgents().run(amain())
```



examples/llm_example.py
```python
"""
Code example for using LLMs.
"""

from pprint import pprint

from dotenv import load_dotenv

from miniagents.ext.llm.openai import openai_agent
from miniagents.messages import Message
from miniagents.miniagents import MiniAgents

load_dotenv()

# logging.basicConfig(level=logging.DEBUG)

# llm_agent = anthropic_agent.fork(model="claude-3-haiku-20240307")  # claude-3-opus-20240229
llm_agent = openai_agent.fork(model="gpt-4o-2024-05-13")  # gpt-3.5-turbo-0125

mini_agents = MiniAgents()


@mini_agents.on_persist_message
async def persist_message(_, message: Message) -> None:
    """
    Print the message to the console.
    """
    print("HASH KEY:", message.hash_key)
    print(type(message).__name__)
    pprint(message.serialize(), width=119)
    print()


async def amain() -> None:
    """
    Send a message to Claude and print the response.
    """
    reply_sequence = llm_agent.inquire(
        "How are you today?",
        max_tokens=1000,
        temperature=0.0,
        system="Respond only in Yoda-speak.",
    )

    print()
    async for msg_promise in reply_sequence:
        async for token in msg_promise:
            print(f"\033[92;1m{token}\033[0m", end="", flush=True)
        print()
        print()


if __name__ == "__main__":
    mini_agents.run(amain())
```



examples/self_dev/docu_agents.py
```python
"""
This module contains MiniAgents that specialize in producing documentation for the MiniAgents framework.
"""

import asyncio
import logging
from pathlib import Path
from typing import Union

from examples.self_dev.self_dev_common import MODEL_AGENTS, SELF_DEV_OUTPUT, SKIPS_FOR_REPO_VARIATIONS, FullRepoMessage
from examples.self_dev.self_dev_prompts import GLOBAL_SYSTEM_HEADER, PRODUCE_README_SYSTEM_FOOTER
from miniagents.ext.llm.llm_common import SystemMessage
from miniagents.miniagents import miniagent, MiniAgents, InteractionContext


@miniagent
async def echo_agent(ctx: InteractionContext, color: Union[str, int] = "92;1") -> None:
    """
    MiniAgent that echoes messages to the console token by token.
    """
    ctx.reply(ctx.messages)  # pass the same messages forward

    async for msg_promise in ctx.messages:
        async for token in msg_promise:
            print(f"\033[{color};1m{token}\033[0m", end="", flush=True)
        print()
        print()


@miniagent
async def file_agent(ctx: InteractionContext, file: str) -> None:
    """
    MiniAgent that writes messages to a file.
    """
    ctx.reply(ctx.messages)  # pass the same messages forward

    file = Path(file)
    file.parent.mkdir(parents=True, exist_ok=True)

    with file.open("w", encoding="utf-8") as file_stream:
        async for readme_token in ctx.messages.as_single_promise():
            file_stream.write(readme_token)


@miniagent
async def readme_agent(_) -> None:  # TODO Oleksandr: make it possible not to specify `_` in the signature ?
    """
    MiniAgent that produces variants of README using different large language models.
    """
    experiment_name = input("\nEnter experiment folder name: ")
    experiment_name = "_".join(experiment_name.lower().split())
    if not experiment_name:
        experiment_name = "DEFAULT"

    # try all repo variations simultaneously
    for variation_idx, (variation_name, variation_skips) in enumerate(SKIPS_FOR_REPO_VARIATIONS.items()):

        # TODO Oleksandr: implement LLM agent throttling

        # start all model agents in parallel
        for model_idx, (model, model_agent) in enumerate(MODEL_AGENTS.items()):

            md_file = SELF_DEV_OUTPUT / f"README__{experiment_name}__{variation_name}__{model}.md"

            if md_file.exists() and md_file.stat().st_size > 0 and md_file.read_text(encoding="utf-8").strip():
                continue

            echo_agent.inquire(
                file_agent.inquire(
                    model_agent.inquire(
                        [
                            SystemMessage(GLOBAL_SYSTEM_HEADER),
                            FullRepoMessage(
                                experiment_name=experiment_name,
                                variation_name=variation_name,
                                skip_if_starts_with=variation_skips,
                            ),
                            SystemMessage(PRODUCE_README_SYSTEM_FOOTER),
                        ],
                        temperature=0,
                    ),
                    file=str(md_file),
                ),
                color=f"{90 + len(MODEL_AGENTS) * variation_idx + model_idx};1",
            )

    # TODO Oleksandr: support this feature
    # await ctx.await_children()


async def amain() -> None:
    """
    Main function that runs the agents.
    """
    async with MiniAgents():
        readme_agent.inquire()
    print("Readme file(s) produced\n")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    logging.getLogger("miniagents.ext.llm").setLevel(logging.DEBUG)

    asyncio.run(amain())
```



examples/self_dev/self_dev_common.py
```python
"""
This module contains common code for the self-developer example.
"""

from pathlib import Path
from typing import Iterable

from dotenv import load_dotenv

from miniagents.ext.llm.anthropic import anthropic_agent
from miniagents.messages import Message

load_dotenv()

MODEL_AGENT_FACTORIES = {
    # "gpt-4o-2024-05-13": openai_agent,
    "claude-3-5-sonnet-20240620": anthropic_agent.fork(max_tokens=2000),
    # "claude-3-opus-20240229": anthropic_agent.fork(max_tokens=2000),
    # "claude-3-haiku-20240307": anthropic_agent.fork(max_tokens=2000),
}
MODEL_AGENTS = {model: agent.fork(model=model) for model, agent in MODEL_AGENT_FACTORIES.items()}

SELF_DEV_ROOT = Path(__file__).parent
MINIAGENTS_ROOT = SELF_DEV_ROOT.parent.parent

SELF_DEV_OUTPUT = SELF_DEV_ROOT / "output"
SELF_DEV_PROMPTS = SELF_DEV_ROOT / "self_dev_prompts.py"
SELF_DEV_TRANSIENT = SELF_DEV_ROOT / "transient"


class RepoFileMessage(Message):
    """
    A message that represents a file in the MiniAgents repository.
    """

    file_posix_path: str

    def _as_string(self) -> str:
        snippet_type = "python" if self.file_posix_path.endswith(".py") else ""
        extra_newline = "" if self.text.endswith("\n") else "\n"
        return f"{self.file_posix_path}\n```{snippet_type}\n{self.text}{extra_newline}```"


SKIPS_FOR_REPO_VARIATIONS: dict[str, list[str]] = {
    "complete": [],
    "no_pypr": ["pyproject.toml"],
    "no_pypr_no_readme": ["pyproject.toml", "README.md"],
    "no_pypr_no_readme_no_examples": ["pyproject.toml", "README.md", "examples/"],
    "no_pypr_no_readme_no_examples_no_tests": ["pyproject.toml", "README.md", "examples/", "tests/"],
}


class FullRepoMessage(Message):  # TODO Oleksandr: bring back `ModelSingleton` ?
    """
    A message that represents the full content of the MiniAgents repository.
    """

    variation_name: str
    repo_files: tuple[RepoFileMessage, ...]

    def __init__(self, experiment_name: str, variation_name: str, skip_if_starts_with: Iterable[str] = ()) -> None:
        """
        Create a FullRepoMessage object that contains the full content of the MiniAgents repository. (Take a snapshot
        of the files as they currently are, in other words.)
        """
        miniagent_files = [
            (relative_posix_path(file), file)
            for file in MINIAGENTS_ROOT.rglob("*")
            if file.is_file() and file.stat().st_size > 0
        ]
        miniagent_files = [
            RepoFileMessage(file_posix_path=file_posix_path, text=file.read_text(encoding="utf-8"))
            for file_posix_path, file in miniagent_files
            if (
                not any(
                    file_posix_path.startswith(prefix)
                    for prefix in [
                        ".",
                        "dist/",
                        relative_posix_path(SELF_DEV_OUTPUT),
                        # relative_posix_path(SELF_DEV_PROMPTS),  # skip prompt file in order not to throw LLM off ?
                        relative_posix_path(SELF_DEV_TRANSIENT),
                        "htmlcov/",
                        "LICENSE",  # TODO Oleksandr: what if there is a `LICENSE-template` file, for ex. ?
                        "venv/",
                        "poetry.lock",
                        *skip_if_starts_with,
                    ]
                )
                and not any(file_posix_path.endswith(suffix) for suffix in [".pyc"])
            )
        ]
        miniagent_files.sort(key=lambda file_message: file_message.file_posix_path)
        super().__init__(repo_files=miniagent_files, variation_name=variation_name)

        full_repo_md_file = SELF_DEV_TRANSIENT / f"REPO__{experiment_name}__{variation_name}.md"
        full_repo_md_file.parent.mkdir(parents=True, exist_ok=True)
        full_repo_md_file.write_text(str(self), encoding="utf-8")

    def _as_string(self) -> str:
        miniagent_files_str = "\n".join([file_message.file_posix_path for file_message in self.repo_files])

        return "\n\n\n\n".join(
            [
                f"File list:\n```\n{miniagent_files_str}\n```",
                *[str(file_message) for file_message in self.repo_files],
            ]
        )


def relative_posix_path(file: Path) -> str:
    """
    Get the path of a file as a POSIX path relative to the MiniAgents repository root.
    """
    return file.relative_to(MINIAGENTS_ROOT).as_posix()


if __name__ == "__main__":
    FullRepoMessage(experiment_name="test", variation_name="complete")
    print("FullRepoMessage created and saved")
```



examples/self_dev/self_dev_prompts.py
```python
"""
This file contains the prompts for the self-developer example.
"""

GLOBAL_SYSTEM_HEADER = (
    "Here are the source files of a Python framework that I'm building that goes by the name of MiniAgents."
)
PRODUCE_README_SYSTEM_FOOTER = "Please produce README.md for this framework."
```



miniagents/chat_history.py
```python
"""
This module contains abstractions for chat history management.
"""

from abc import ABC, abstractmethod
from functools import cached_property

from miniagents.messages import Message
from miniagents.miniagent_typing import MessageType
from miniagents.miniagents import InteractionContext, miniagent, MiniAgent, MessageSequence


class ChatHistory(ABC):
    """
    Abstract class for loading chat history from a storage as well as writing new messages to it.
    """

    @cached_property
    def logging_agent(self) -> MiniAgent:
        """
        The agent that logs the chat history to a storage. Replies with the same messages for agent
        chaining purposes.
        """
        return miniagent(self._logging_agent_chained)

    @abstractmethod
    async def aload_chat_history(self) -> tuple[Message]:
        """
        Load the chat history from the storage.
        """

    @abstractmethod
    async def _logging_agent(self, ctx: InteractionContext) -> None:
        """
        The implementation of the agent that logs the chat history to a storage.
        """

    async def _logging_agent_chained(self, ctx: InteractionContext) -> None:
        """
        The implementation of the agent that logs the chat history to a storage.

        ATTENTION! Apart for logging the messages, it also replies with the same messages for agent
        chaining purposes.
        """
        ctx.reply(ctx.messages)  # asynchronously(!) reply with the same messages for agent chaining purposes
        await self._logging_agent(ctx)


class InMemoryChatHistory(ChatHistory):
    """
    Class for loading chat history from memory as well as writing new messages to it.
    """

    def __init__(self) -> None:
        self._chat_history: list[MessageType] = []

    async def _logging_agent(self, ctx: InteractionContext) -> None:
        """
        The implementation of the agent that logs the chat history to memory.
        """
        self._chat_history.append(ctx.messages)

    async def aload_chat_history(self) -> tuple[Message, ...]:
        """
        Load the chat history from memory.
        """
        return await MessageSequence.aresolve_messages(self._chat_history)
```



miniagents/ext/chat_history_md.py
```python
"""
This module provides a class working with chat history stored in a markdown file.
"""

from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Union

from markdown_it import MarkdownIt

from miniagents.chat_history import ChatHistory
from miniagents.messages import Message
from miniagents.miniagents import InteractionContext


class ChatHistoryMD(ChatHistory):
    """
    Class for loading chat history from a markdown file as well as writing new messages to it.
    """

    _md = MarkdownIt()

    def __init__(self, chat_md_file: Union[str, Path], default_role: str = "assistant") -> None:
        self._chat_md_file = Path(chat_md_file)
        self._default_role = default_role

    async def _logging_agent(self, ctx: InteractionContext) -> None:
        """
        The implementation of the agent that logs the chat history to a markdown file.
        """
        with self._chat_md_file.open(
            "a",  # append mode
            buffering=1,  # line buffering
            encoding="utf-8",
        ) as chat_md_file:
            async for msg_promise in ctx.messages:
                try:
                    message_role = msg_promise.preliminary_metadata.role
                except AttributeError:
                    message_role = self._default_role
                try:
                    message_model = f" / {msg_promise.preliminary_metadata.model}"
                except AttributeError:
                    message_model = ""

                chat_md_file.write(f"\n{message_role}{message_model}\n========================================\n")

                async for token in msg_promise:
                    chat_md_file.write(token)
                chat_md_file.write("\n")

    async def aload_chat_history(self) -> tuple[Message, ...]:
        """
        Parse a markdown content as a dialog.
        TODO Oleksandr: implement exhaustive unit tests for this function
        """
        md_content = self._chat_md_file.read_text(encoding="utf-8")

        md_lines = md_content.split("\n")
        md_tokens = self._md.parse(md_content)

        last_section = None
        sections = []

        for idx, md_token in enumerate(md_tokens):
            if md_token.type != "heading_open" or md_token.tag != "h1" or md_token.level != 0:
                continue

            heading = md_tokens[idx + 1].content  # the next token is `inline` with the heading content
            heading_parts = heading.split("/", maxsplit=1)
            role = heading_parts[0].strip().lower()

            if role not in ["user", "assistant"]:
                continue

            if len(heading_parts) > 1:
                model = heading_parts[1].strip()
                if any(c.isspace() for c in model):
                    # model name should not contain whitespaces - this heading is probably not really a role heading
                    continue
            else:
                model = None

            if last_section:
                last_section.content = self._grab_and_clean_up_lines(
                    md_lines, last_section.content_start_line, md_token.map[0]
                )
                sections.append(last_section)

            last_section = self._Section(
                role=role,
                model=model,
                content_start_line=md_token.map[1],
            )

        if last_section:
            last_section.content = self._grab_and_clean_up_lines(md_lines, last_section.content_start_line)
            sections.append(last_section)

        return tuple(Message(role=section.role, model=section.model, text=section.content) for section in sections)

    @staticmethod
    def _grab_and_clean_up_lines(md_lines: list[str], start_line: int, end_line: Optional[int] = None) -> str:
        """
        Grab a snippet of the markdown content by start and end line numbers and clean it up (remove leading
        and trailing empty lines).
        """
        if end_line is None:
            end_line = len(md_lines)

        content_lines = md_lines[start_line:end_line]

        # remove leading and trailing empty lines (but keep the leading and trailing whitespaces of the
        # non-empty lines)
        while content_lines and not content_lines[0].strip():
            content_lines.pop(0)
        while content_lines and not content_lines[-1].strip():
            content_lines.pop()

        if not content_lines:
            # there is no content in this section
            return ""

        return "\n".join(content_lines)

    @dataclass
    class _Section:
        """
        Represents a section of the markdown content.
        """

        role: str
        model: Optional[str]
        content_start_line: int
        content: Optional[str] = None
```



miniagents/ext/console_user_agent.py
```python
"""
This module provides a user agent that reads user input from the console, writes back to the console and also keeps
track of the chat history using the provided ChatHistory object.
"""

from typing import Optional

from prompt_toolkit import PromptSession, HTML
from prompt_toolkit.document import Document
from prompt_toolkit.key_binding import KeyBindings
from prompt_toolkit.keys import Keys
from prompt_toolkit.lexers import Lexer
from prompt_toolkit.styles import Style

from miniagents.chat_history import ChatHistory, InMemoryChatHistory
from miniagents.ext.llm.llm_common import UserMessage
from miniagents.miniagents import miniagent, InteractionContext

GLOBAL_CHAT_HISTORY = InMemoryChatHistory()


@miniagent
async def console_user_agent(ctx: InteractionContext, chat_history: Optional[ChatHistory] = None) -> None:
    """
    User agent that reads user input from the console, writes back to the console and also keeps track of
    the chat history using the provided ChatHistory object.
    """
    if chat_history is None:
        chat_history = GLOBAL_CHAT_HISTORY

    # technically `input_messages` are going to be the same as `ctx.messages`, but reading them instead of the
    # original `ctx.messages` ensures that all these messages will be logged to the chat history by the time
    # we are done iterating over `input_messages` here (because our async loop here will have to wait for the
    # `logging_agent` to finish in order to be sure that these are all the messages there are in `logging_agent`
    # response)
    input_messages = chat_history.logging_agent.inquire(ctx.messages)

    assistant_style = "\033[92;1m"
    cancel_style = "\033[0m"

    async for msg_promise in input_messages:
        print(f"\n{assistant_style}{msg_promise.preliminary_metadata.agent_alias}: {cancel_style}", end="", flush=True)
        async for token in msg_promise:
            print(f"{assistant_style}{token}{cancel_style}", end="", flush=True)
        print("\n")  # this produces a double newline after a single message

    # TODO Oleksandr: should MessageSequencePromise support `cancel()` operation
    #  (to interrupt whoever is producing it) ?

    # TODO Oleksandr: mention that ctrl+space is used to insert a newline ?
    user_input = await _prompt_session.prompt_async(
        HTML("<user_utterance>USER: </user_utterance>"),
        multiline=True,
        key_bindings=_prompt_bindings,
        lexer=_CustomPromptLexer(),
        style=_user_prompt_style,
    )
    # the await below makes sure that writing to the chat history is finished before we proceed to reading it back
    await chat_history.logging_agent.inquire(UserMessage(user_input))

    chat_history = await chat_history.aload_chat_history()
    ctx.reply(chat_history)


_user_prompt_style = Style.from_dict({"user_utterance": "fg:ansibrightyellow bold"})

_prompt_session = PromptSession()

_prompt_bindings = KeyBindings()


@_prompt_bindings.add(Keys.Enter)
def _prompt_binding_enter(event):
    event.current_buffer.validate_and_handle()


@_prompt_bindings.add(Keys.ControlSpace)
def _prompt_binding_control_space(event):
    event.current_buffer.insert_text("\n")


class _CustomPromptLexer(Lexer):
    """
    Custom lexer that paints user utterances in yellow (and bold).
    """

    def lex_document(self, document: Document):
        """
        Lex the document.
        """
        return lambda i: [("class:user_utterance", document.text.split("\n")[i])]
```



miniagents/ext/llm/anthropic.py
```python
# pylint: disable=duplicate-code
"""
This module integrates Anthropic language models with MiniAgents.
"""

import logging
import typing
from functools import cache
from pprint import pformat
from typing import AsyncIterator, Any, Optional

from anthropic import NOT_GIVEN

from miniagents.ext.llm.llm_common import message_to_llm_dict, AssistantMessage
from miniagents.miniagents import miniagent, MiniAgents, InteractionContext

if typing.TYPE_CHECKING:
    import anthropic as anthropic_original

logger = logging.getLogger(__name__)


class AnthropicMessage(AssistantMessage):
    """
    A message generated by an Anthropic model.
    """


@miniagent
async def anthropic_agent(
    ctx: InteractionContext,
    model: str,
    stream: Optional[bool] = None,
    system: Optional[str] = None,
    fake_first_user_message: str = "/start",
    message_delimiter_for_same_role: str = "\n\n",
    async_client: Optional["anthropic_original.AsyncAnthropic"] = None,
    reply_metadata: Optional[dict[str, Any]] = None,
    **kwargs,
) -> None:
    """
    An agent that represents Large Language Models by Anthropic.
    """
    if not async_client:
        async_client = _default_anthropic_client()

    if stream is None:
        stream = MiniAgents.get_current().stream_llm_tokens_by_default

    async def message_token_streamer(metadata_so_far: dict[str, Any]) -> AsyncIterator[str]:
        resolved_messages = await ctx.messages.aresolve_messages()

        message_dicts = [message_to_llm_dict(msg) for msg in resolved_messages]
        message_dicts = _fix_message_dicts(
            message_dicts,
            fake_first_user_message=fake_first_user_message,
            message_delimiter_for_same_role=message_delimiter_for_same_role,
        )

        if message_dicts and message_dicts[-1]["role"] == "system":
            # let's strip away the system message at the end
            system_message_dict = message_dicts.pop()
            system_combined = (
                system_message_dict["content"]
                if system is None
                else f"{system}{message_delimiter_for_same_role}{system_message_dict['content']}"
            )
        else:
            system_combined = system

        if system_combined is None:
            system_combined = NOT_GIVEN

        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(
                "SENDING TO ANTHROPIC:\n\n%s\nSYSTEM:\n%s\n", pformat(message_dicts), pformat(system_combined)
            )

        if stream:
            # pylint: disable=not-async-context-manager
            async with async_client.messages.stream(
                messages=message_dicts, system=system_combined, model=model, **kwargs
            ) as response:
                async for token in response.text_stream:
                    yield token
                anthropic_final_message = await response.get_final_message()
        else:
            anthropic_final_message = await async_client.messages.create(
                messages=message_dicts, stream=False, system=system_combined, model=model, **kwargs
            )
            if len(anthropic_final_message.content) != 1:
                raise RuntimeError(
                    f"exactly one TextBlock was expected from Anthropic, "
                    f"but {len(anthropic_final_message.content)} were returned instead"
                )
            yield anthropic_final_message.content[0].text  # yield the whole text as one "piece"

        metadata_so_far.update(anthropic_final_message.model_dump(exclude={"content"}))

    ctx.reply(
        AnthropicMessage.promise(
            start_asap=True,  # TODO Oleksandr: should this be customizable ?
            message_token_streamer=message_token_streamer,
            # preliminary metadata:
            model=model,
            agent_alias=ctx.this_agent.alias,
            **(reply_metadata or {}),
        )
    )


@cache
def _default_anthropic_client() -> "anthropic_original.AsyncAnthropic":
    # pylint: disable=import-outside-toplevel
    # noinspection PyShadowingNames
    import anthropic as anthropic_original

    return anthropic_original.AsyncAnthropic()


def _fix_message_dicts(
    message_dicts: list[dict[str, Any]], fake_first_user_message: str, message_delimiter_for_same_role: str
) -> list[dict[str, Any]]:
    if not message_dicts:
        return []

    # let's put all the system messages at the end (they will be stripped away)
    non_system_message_dicts = [message_dict for message_dict in message_dicts if message_dict["role"] != "system"]
    system_message_dicts = [message_dict for message_dict in message_dicts if message_dict["role"] == "system"]
    message_dicts = non_system_message_dicts + system_message_dicts

    fixed_message_dicts = []
    if message_dicts[0]["role"] != "user":
        # Anthropic requires the first message to come from the user (system messages don't count - their content
        # will go into a separate, `system` parameter of the API call)
        fixed_message_dicts.append({"role": "user", "content": fake_first_user_message})

    # if multiple messages with the same role are sent in a row, they should be concatenated
    for message_dict in message_dicts:
        if fixed_message_dicts and message_dict["role"] == fixed_message_dicts[-1]["role"]:
            fixed_message_dicts[-1]["content"] += message_delimiter_for_same_role + message_dict["content"]
        else:
            fixed_message_dicts.append(message_dict)

    return fixed_message_dicts
```



miniagents/ext/llm/llm_common.py
```python
"""
Common classes and functions for working with large language models.
"""

from typing import Any, Optional

from miniagents.messages import Message


class UserMessage(Message):
    """
    A message from a user.
    """

    role: str = "user"


class SystemMessage(Message):
    """
    A message that is marked as a system message (a concept in large language models).
    """

    role: str = "system"


class AssistantMessage(Message):
    """
    A message generated by a large language model.
    """

    role: str = "assistant"
    model: Optional[str] = None
    agent_alias: Optional[str] = None


def message_to_llm_dict(message: Message) -> dict[str, Any]:
    """
    Convert a message to a dictionary that can be sent to a large language model.
    """
    try:
        role = message.role
    except AttributeError:
        role = "user"

    return {
        "role": role,
        "content": str(message),
    }
```



miniagents/ext/llm/openai.py
```python
# pylint: disable=duplicate-code
"""
This module integrates OpenAI language models with MiniAgents.
"""

import logging
import typing
from functools import cache
from pprint import pformat
from typing import AsyncIterator, Any, Optional

from miniagents.ext.llm.llm_common import message_to_llm_dict, AssistantMessage
from miniagents.miniagents import (
    miniagent,
    MiniAgents,
    InteractionContext,
)

if typing.TYPE_CHECKING:
    import openai as openai_original

logger = logging.getLogger(__name__)


class OpenAIMessage(AssistantMessage):
    """
    A message generated by an OpenAI model.
    """


@miniagent
async def openai_agent(
    ctx: InteractionContext,
    model: str,
    stream: Optional[bool] = None,
    system: Optional[str] = None,
    n: int = 1,
    async_client: Optional["openai_original.AsyncOpenAI"] = None,
    reply_metadata: Optional[dict[str, Any]] = None,
    **kwargs,
) -> None:
    """
    An agent that represents Large Language Models by OpenAI.
    """
    if not async_client:
        async_client = _default_openai_client()

    if stream is None:
        stream = MiniAgents.get_current().stream_llm_tokens_by_default

    if n != 1:
        raise ValueError("Only n=1 is supported by MiniAgents for AsyncOpenAI().chat.completions.create()")

    async def message_token_streamer(metadata_so_far: dict[str, Any]) -> AsyncIterator[str]:
        resolved_messages = await ctx.messages.aresolve_messages()

        if system is None:
            message_dicts = []
        else:
            message_dicts = [
                {
                    "role": "system",
                    "content": system,
                },
            ]
        message_dicts.extend(message_to_llm_dict(msg) for msg in resolved_messages)

        if logger.isEnabledFor(logging.DEBUG):
            logger.debug("SENDING TO OPENAI:\n\n%s\n", pformat(message_dicts))

        openai_response = await async_client.chat.completions.create(
            messages=message_dicts, model=model, stream=stream, **kwargs
        )
        if stream:
            async for chunk in openai_response:
                if len(chunk.choices) != 1:  # TODO Oleksandr: do I really need to check it for every token ?
                    raise RuntimeError(
                        f"exactly one Choice was expected from OpenAI, "
                        f"but {len(openai_response.choices)} were returned instead"
                    )
                token = chunk.choices[0].delta.content
                if token:
                    yield token

                metadata_so_far["role"] = chunk.choices[0].delta.role or metadata_so_far["role"]
                _merge_openai_dicts(
                    metadata_so_far,
                    chunk.model_dump(exclude={"choices": {0: {"index": ..., "delta": {"content": ..., "role": ...}}}}),
                )
        else:
            if len(openai_response.choices) != 1:
                raise RuntimeError(
                    f"exactly one Choice was expected from OpenAI, "
                    f"but {len(openai_response.choices)} were returned instead"
                )
            yield openai_response.choices[0].message.content  # yield the whole text as one "piece"

            metadata_so_far["role"] = openai_response.choices[0].message.role
            metadata_so_far.update(
                openai_response.model_dump(
                    exclude={"choices": {0: {"index": ..., "message": {"content": ..., "role": ...}}}}
                )
            )

    ctx.reply(
        OpenAIMessage.promise(
            start_asap=True,  # TODO Oleksandr: should this be customizable ?
            message_token_streamer=message_token_streamer,
            # preliminary metadata:
            model=model,
            agent_alias=ctx.this_agent.alias,
            **(reply_metadata or {}),
        )
    )


@cache
def _default_openai_client() -> "openai_original.AsyncOpenAI":
    # pylint: disable=import-outside-toplevel
    # noinspection PyShadowingNames
    import openai as openai_original

    return openai_original.AsyncOpenAI()


def _merge_openai_dicts(destination_dict: dict[str, Any], dict_to_merge: dict[str, Any]) -> None:
    """
    Merge the dict_to_merge into the destination_dict.
    """
    for key, value in dict_to_merge.items():
        if value is not None:
            existing_value = destination_dict.get(key)
            if isinstance(existing_value, dict):
                _merge_openai_dicts(existing_value, value)
            elif isinstance(existing_value, list):
                if key == "choices":
                    if not existing_value:
                        destination_dict[key] = [{}]  # we only expect a single choice in our implementation
                    _merge_openai_dicts(destination_dict[key][0], value[0])
                else:
                    destination_dict[key].extend(value)
            else:
                destination_dict[key] = value
```



miniagents/messages.py
```python
"""
`Message` class and other classes related to messages.
"""

from functools import cached_property
from typing import AsyncIterator, Any, Union, Optional, Iterator

from miniagents.miniagent_typing import MessageTokenStreamer
from miniagents.promising.ext.frozen import Frozen
from miniagents.promising.promising import StreamedPromise
from miniagents.promising.sentinels import Sentinel, DEFAULT


class Message(Frozen):
    """
    A message that can be sent between agents.
    """

    text: Optional[str] = None
    text_template: Optional[str] = None

    @cached_property
    def as_promise(self) -> "MessagePromise":
        """
        Convert this message into a MessagePromise object.
        """
        return MessagePromise(prefill_message=self)

    @classmethod
    def promise(
        cls,
        start_asap: Union[bool, Sentinel] = DEFAULT,
        message_token_streamer: Optional[MessageTokenStreamer] = None,
        **preliminary_metadata,
    ) -> "MessagePromise":
        """
        Create a MessagePromise object based on the Message class this method is called for and the provided
        arguments.
        """
        if message_token_streamer:
            return MessagePromise(
                start_asap=start_asap,
                message_token_streamer=message_token_streamer,
                message_class=cls,
                **preliminary_metadata,
            )
        return cls(**preliminary_metadata).as_promise

    def serialize(self) -> dict[str, Any]:
        include_into_serialization, sub_messages = self._serialization_metadata
        model_dump = self.model_dump(include=include_into_serialization)

        for path, message_or_messages in sub_messages.items():
            sub_dict = model_dump
            for path_part in path[:-1]:
                sub_dict = sub_dict[path_part]
            if isinstance(message_or_messages, Message):
                sub_dict[f"{path[-1]}__hash_key"] = message_or_messages.hash_key
            else:
                sub_dict[f"{path[-1]}__hash_keys"] = tuple(message.hash_key for message in message_or_messages)
        return model_dump

    def sub_messages(self) -> Iterator["Message"]:
        """
        Iterate over all sub-messages of this message, no matter how deep they are nested. This is a depth-first
        traversal.
        """
        _, sub_messages = self._serialization_metadata
        for _, message_or_messages in sub_messages.items():
            if isinstance(message_or_messages, Message):
                yield from message_or_messages.sub_messages()
                yield message_or_messages
            else:
                for message in message_or_messages:
                    yield from message.sub_messages()
                    yield message

    @cached_property
    def _serialization_metadata(
        self,
    ) -> tuple[
        dict[Union[str, int], Any],
        dict[tuple[Union[str, int], ...], Union["Message", tuple["Message", ...]]],
    ]:
        include_into_serialization = {}
        sub_messages = {}

        def build_serialization_metadata(
            inclusion_dict: dict[Union[str, int], Any],
            node: Frozen,
            node_path: tuple[Union[str, int], ...],
        ) -> None:
            # pylint: disable=protected-access
            for field, value in node._frozen_fields_and_values(exclude_class=False):
                if isinstance(value, Message):
                    sub_messages[(*node_path, field)] = value

                elif isinstance(value, Frozen):
                    sub_dict = {}
                    build_serialization_metadata(sub_dict, value, (*node_path, field))
                    inclusion_dict[field] = sub_dict

                elif isinstance(value, tuple):
                    if value and isinstance(value[0], Message):
                        # TODO Oleksandr: introduce a concept of MessageRef to also support "mixed" tuples (with
                        #  both Messages and other types of values mixed together)
                        sub_messages[(*node_path, field)] = value

                    else:
                        sub_dict = {}
                        for idx, sub_value in enumerate(value):
                            if isinstance(sub_value, Frozen):
                                sub_sub_dict = {}
                                build_serialization_metadata(sub_sub_dict, sub_value, (*node_path, field, idx))
                                sub_dict[idx] = sub_sub_dict
                            else:
                                sub_dict[idx] = ...
                        inclusion_dict[field] = sub_dict

                else:
                    # any other (primitive) type of value will be included into serialization in its entirety
                    inclusion_dict[field] = ...

        build_serialization_metadata(include_into_serialization, self, ())
        return include_into_serialization, sub_messages

    def _as_string(self) -> str:
        if self.text is not None:
            return self.text
        if self.text_template is not None:
            # TODO Oleksandr: exclude_class=False ?
            return self.text_template.format(**self.frozen_fields_and_values())
        return super()._as_string()

    def __init__(self, text: Optional[str] = None, **metadata: Any) -> None:
        super().__init__(text=text, **metadata)
        self._persist_message_event_triggered = False


class MessagePromise(StreamedPromise[str, Message]):
    """
    A promise of a message that can be streamed token by token.
    """

    preliminary_metadata: Frozen

    def __init__(
        self,
        start_asap: Union[bool, Sentinel] = DEFAULT,
        message_token_streamer: Optional[MessageTokenStreamer] = None,
        prefill_message: Optional[Message] = None,
        message_class: type[Message] = Message,
        **preliminary_metadata,
    ) -> None:
        # TODO Oleksandr: raise an error if both ready_message and message_token_streamer/preliminary_metadata
        #  are not None (or both are None)
        if prefill_message:
            self.preliminary_metadata = prefill_message

            super().__init__(
                start_asap=start_asap,
                prefill_pieces=[str(prefill_message)],
                prefill_result=prefill_message,
            )
        else:
            self.preliminary_metadata = Frozen(**preliminary_metadata)
            self._metadata_so_far = self.preliminary_metadata.frozen_fields_and_values()

            self._message_token_streamer = message_token_streamer
            self._message_class = message_class
            super().__init__(start_asap=start_asap)

    def _streamer(self) -> AsyncIterator[str]:
        return self._message_token_streamer(self._metadata_so_far)

    async def _resolver(self) -> Message:
        return self._message_class(
            text="".join([token async for token in self]),
            **self._metadata_so_far,
        )

    def __aiter__(self) -> AsyncIterator[str]:
        # PyCharm fails to see that MessagePromise inherits AsyncIterable protocol from StreamedPromise,
        # hence the need to explicitly declare the __aiter__ method here
        # TODO Oleksandr: is there any other way to make PyCharm see that this class inherits AsyncIterable ?
        return super().__aiter__()


class MessageSequencePromise(StreamedPromise[MessagePromise, tuple[MessagePromise, ...]]):
    """
    A promise of a sequence of messages that can be streamed message by message.
    """

    async def aresolve_messages(self) -> tuple[Message, ...]:
        """
        Resolve all the messages in the sequence (which also includes collecting all the streamed tokens)
        and return them as a tuple of Message objects.
        """
        # pylint: disable=consider-using-generator
        return tuple([await message_promise async for message_promise in self])

    def __aiter__(self) -> AsyncIterator[MessagePromise]:
        # PyCharm fails to see that MessageSequencePromise inherits AsyncIterable protocol from StreamedPromise,
        # hence the need to explicitly declare the __aiter__ method here
        # TODO Oleksandr: is there any other way to make PyCharm see that this class inherits AsyncIterable ?
        return super().__aiter__()

    def as_single_promise(self, **kwargs) -> MessagePromise:
        """
        Convert this sequence promise into a single message promise that will contain all the messages from this
        sequence (separated by double newlines by default).
        """
        from miniagents.utils import join_messages  # pylint: disable=import-outside-toplevel

        return join_messages(self, start_asap=False, **kwargs)
```



miniagents/miniagent_typing.py
```python
"""
Types of the MiniAgents framework.
"""

import typing
from typing import AsyncIterator, Protocol, Union, Any, Iterable, AsyncIterable

from pydantic import BaseModel

from miniagents.promising.promise_typing import PromiseBound

if typing.TYPE_CHECKING:
    # noinspection PyUnresolvedReferences
    from miniagents.messages import Message, MessagePromise
    from miniagents.miniagents import InteractionContext


class AgentFunction(Protocol):
    """
    A protocol for agent functions.
    """

    async def __call__(self, ctx: "InteractionContext", **kwargs) -> None: ...


class MessageTokenStreamer(Protocol):
    """
    A protocol for message token streamer functions.
    """

    def __call__(self, metadata_so_far: dict[str, Any]) -> AsyncIterator[str]: ...


class PersistMessageEventHandler(Protocol):
    """
    TODO Oleksandr: docstring
    """

    async def __call__(self, promise: PromiseBound, message: "Message") -> None: ...


# TODO Oleksandr: add documentation somewhere that explains what MessageType and SingleMessageType represent
SingleMessageType = Union[str, dict[str, Any], BaseModel, "Message", "MessagePromise", BaseException]
MessageType = Union[SingleMessageType, Iterable["MessageType"], AsyncIterable["MessageType"]]
```



miniagents/miniagents.py
```python
"""
"Core" classes of the MiniAgents framework.
"""

import asyncio
import copy
import logging
from functools import partial
from typing import AsyncIterator, Any, Union, Optional, Callable, Iterable, Awaitable

from pydantic import BaseModel

from miniagents.messages import MessagePromise, MessageSequencePromise, Message
from miniagents.miniagent_typing import MessageType, AgentFunction, PersistMessageEventHandler
from miniagents.promising.ext.frozen import Frozen
from miniagents.promising.promise_typing import PromiseStreamer, PromiseResolvedEventHandler
from miniagents.promising.promising import StreamAppender, Promise, PromisingContext
from miniagents.promising.sentinels import Sentinel, DEFAULT
from miniagents.promising.sequence import FlatSequence

logger = logging.getLogger(__name__)


class MiniAgents(PromisingContext):
    """
    TODO Oleksandr: docstring
    """

    def __init__(
        self,
        stream_llm_tokens_by_default: bool = True,
        on_promise_resolved: Union[PromiseResolvedEventHandler, Iterable[PromiseResolvedEventHandler]] = (),
        on_persist_message: Union[PersistMessageEventHandler, Iterable[PersistMessageEventHandler]] = (),
        **kwargs,
    ) -> None:
        on_promise_resolved = (
            [self._trigger_persist_message_event, on_promise_resolved]
            if callable(on_promise_resolved)
            else [self._trigger_persist_message_event, *on_promise_resolved]
        )
        super().__init__(on_promise_resolved=on_promise_resolved, **kwargs)
        self.stream_llm_tokens_by_default = stream_llm_tokens_by_default
        self.on_persist_message_handlers: list[PersistMessageEventHandler] = (
            [on_persist_message] if callable(on_persist_message) else list(on_persist_message)
        )

    def run(self, awaitable: Awaitable[Any]) -> Any:
        """
        Run an awaitable in the MiniAgents context. This method is blocking. It also creates a new event loop.
        """
        return asyncio.run(self.arun(awaitable))

    async def arun(self, awaitable: Awaitable[Any]) -> Any:
        """
        Run an awaitable in the MiniAgents context.
        """
        async with self:
            return await awaitable

    @classmethod
    def get_current(cls) -> "MiniAgents":
        # noinspection PyTypeChecker
        return super().get_current()

    def on_persist_message(self, handler: PersistMessageEventHandler) -> PersistMessageEventHandler:
        """
        Add a handler that will be called every time a Message needs to be persisted.
        """
        self.on_persist_message_handlers.append(handler)
        return handler

    # noinspection PyProtectedMember
    async def _trigger_persist_message_event(self, _, obj: Any) -> None:
        # pylint: disable=protected-access
        if not isinstance(obj, Message):
            return

        log_level_for_errors = MiniAgents.get_current().log_level_for_errors

        for sub_message in obj.sub_messages():
            if sub_message._persist_message_event_triggered:
                continue

            for handler in self.on_persist_message_handlers:
                self.start_asap(
                    handler(_, sub_message), suppress_errors=True, log_level_for_errors=log_level_for_errors
                )
            sub_message._persist_message_event_triggered = True

        if obj._persist_message_event_triggered:
            return

        for handler in self.on_persist_message_handlers:
            self.start_asap(handler(_, obj), suppress_errors=True, log_level_for_errors=log_level_for_errors)
        obj._persist_message_event_triggered = True


def miniagent(
    func: Optional[AgentFunction] = None,
    alias: Optional[str] = None,
    description: Optional[str] = None,
    uppercase_func_name: bool = True,
    normalize_spaces_in_docstring: bool = True,
    interaction_metadata: Optional[dict[str, Any]] = None,
    **partial_kwargs,
) -> Union["MiniAgent", Callable[[AgentFunction], "MiniAgent"]]:
    """
    A decorator that converts an agent function into an agent.
    """
    if func is None:
        # the decorator `@miniagent(...)` was used with arguments
        def _decorator(f: AgentFunction) -> "MiniAgent":
            return MiniAgent(
                f,
                alias=alias,
                description=description,
                uppercase_func_name=uppercase_func_name,
                normalize_spaces_in_docstring=normalize_spaces_in_docstring,
                interaction_metadata=interaction_metadata,
                **partial_kwargs,
            )

        return _decorator

    # the decorator `@miniagent` was used either without arguments or as a direct function call
    return MiniAgent(
        func,
        alias=alias,
        description=description,
        uppercase_func_name=uppercase_func_name,
        normalize_spaces_in_docstring=normalize_spaces_in_docstring,
        interaction_metadata=interaction_metadata,
        **partial_kwargs,
    )


class MiniAgent:
    """
    A wrapper for an agent function that allows calling the agent.
    """

    def __init__(
        self,
        func: AgentFunction,
        alias: Optional[str] = None,
        description: Optional[str] = None,
        # TODO Oleksandr: use DEFAULT for the following two arguments (and put them into MiniAgents class)
        uppercase_func_name: bool = True,
        normalize_spaces_in_docstring: bool = True,
        interaction_metadata: Optional[dict[str, Any]] = None,
        **partial_kwargs,
    ) -> None:
        self._func = func
        if partial_kwargs:
            # NOTE: we cannot deep-copy the partial_kwargs here, because they may contain objects that are
            # not serializable (for ex. AsyncAnthropic and AsyncOpenAI objects in case of anthropic and openai
            # miniagents)
            self._func = partial(func, **partial_kwargs)

        # validate interaction metadata
        # TODO Oleksandr: is `interaction_metadata` a good name ? see how it is used in Recensia to decide
        self.interaction_metadata = Frozen(**(interaction_metadata or {}))
        self._interact_metadata_dict = self.interaction_metadata.frozen_fields_and_values()

        self.alias = alias
        if self.alias is None:
            self.alias = func.__name__
            if uppercase_func_name:
                self.alias = self.alias.upper()

        self.description = description
        if self.description is None:
            self.description = func.__doc__
            if self.description and normalize_spaces_in_docstring:
                self.description = " ".join(self.description.split())
        if self.description:
            # replace all {AGENT_ALIAS} entries in the description with the actual agent alias
            self.description = self.description.format(AGENT_ALIAS=self.alias)

        self.__name__ = self.alias
        self.__doc__ = self.description

    def inquire(
        self,
        messages: Optional[MessageType] = None,
        start_asap: Union[bool, Sentinel] = DEFAULT,
        **function_kwargs,
    ) -> MessageSequencePromise:
        """
        TODO Oleksandr: docstring
        """
        agent_call = self.initiate_inquiry(start_asap=start_asap, **function_kwargs)
        if messages is not None:
            agent_call.send_message(messages)
        return agent_call.reply_sequence()

    def initiate_inquiry(
        self,
        start_asap: Union[bool, Sentinel] = DEFAULT,
        **function_kwargs,
    ) -> "AgentCall":
        """
        Start an inquiry with the agent. The agent will be called with the provided function kwargs.
        TODO Oleksandr: expand this docstring ?
        """
        input_sequence = MessageSequence(
            start_asap=False,
        )
        reply_sequence = AgentReplyMessageSequence(
            mini_agent=self,
            function_kwargs=function_kwargs,
            input_sequence_promise=input_sequence.sequence_promise,
            start_asap=start_asap,
        )

        agent_call = AgentCall(
            message_streamer=input_sequence.message_appender,
            reply_sequence_promise=reply_sequence.sequence_promise,
        )
        return agent_call

    def fork(
        self,
        alias: Optional[str] = None,  # TODO Oleksandr: enforce unique aliases ? introduce some "fork identifier" ?
        description: Optional[str] = None,
        interaction_metadata: Optional[dict[str, Any]] = None,
        **partial_kwargs,
    ) -> Union["MiniAgent", Callable[[AgentFunction], "MiniAgent"]]:
        """
        TODO Oleksandr: docstring
        """
        return MiniAgent(
            self._func,
            alias=alias or self.alias,
            description=description or self.description,
            uppercase_func_name=False,
            normalize_spaces_in_docstring=False,
            interaction_metadata={**self._interact_metadata_dict, **(interaction_metadata or {})},
            **partial_kwargs,
        )


class InteractionContext:
    """
    TODO Oleksandr: docstring
    """

    def __init__(
        self, this_agent: "MiniAgent", messages: MessageSequencePromise, reply_streamer: StreamAppender[MessageType]
    ) -> None:
        self.this_agent = this_agent
        self.messages = messages
        self._reply_streamer = reply_streamer

    def reply(self, messages: MessageType) -> None:
        """
        Send a reply to the messages that were received by the agent. The messages can be of any allowed MessageType.
        They will be converted to Message objects when they arrive at the agent that sent the original messages.
        """
        # TODO Oleksandr: add a warning that iterators, async iterators and generators, if passed as `messages` will
        #  not be iterated over immediately, which means that if two agent calls are passed as a generator, those
        #  agent calls will not be scheduled for parallel execution, unless the generator is wrapped into a list (to
        #  guarantee that it will be iterated over immediately)
        # TODO Oleksandr: implement a utility in MiniAgents that deep-copies/freezes mutable data containers
        #  while keeping objects of other types intact and use it in StreamAppender to freeze the state of those
        #  objects upon their submission (this way the user will not have to worry about things like `history[:]`
        #  in the code below)
        self._reply_streamer.append(messages)

    def finish_early(self) -> None:  # TODO Oleksandr: is this a good name for this method ?
        """
        TODO Oleksandr: docstring
        """
        # TODO Oleksandr: what to do with exceptions in agent function that may happen after this method was called ?
        self._reply_streamer.close()


class AgentCall:
    """
    TODO Oleksandr: docstring
    """

    def __init__(
        self,
        message_streamer: StreamAppender[MessageType],
        reply_sequence_promise: MessageSequencePromise,
    ) -> None:
        self._message_streamer = message_streamer
        self._reply_sequence_promise = reply_sequence_promise

        self._message_streamer.open()

    def send_message(self, message: MessageType) -> "AgentCall":
        """
        Send an input message to the agent.
        """
        self._message_streamer.append(message)
        return self

    def reply_sequence(self) -> MessageSequencePromise:
        """
        Finish the agent call and return the agent's response(s).

        NOTE: After this method is called it is not possible to send any more requests to this AgentCall object.
        """
        self.finish()
        return self._reply_sequence_promise

    def finish(self) -> "AgentCall":
        """
        Finish the agent call.

        NOTE: After this method is called it is not possible to send any more requests to this AgentCall object.
        """
        # TODO Oleksandr: also make sure to close the streamer when the parent agent call is finished
        self._message_streamer.close()
        return self


class AgentInteractionNode(Message):
    """
    TODO Oleksandr: docstring
    """

    agent_alias: str


class AgentCallNode(AgentInteractionNode):
    """
    TODO Oleksandr: docstring
    """

    messages: tuple[Message, ...]


class AgentReplyNode(AgentInteractionNode):
    """
    TODO Oleksandr: docstring
    """

    agent_call: AgentCallNode
    replies: tuple[Message, ...]


class MessageSequence(FlatSequence[MessageType, MessagePromise]):
    """
    TODO Oleksandr: docstring
    """

    message_appender: Optional[StreamAppender[MessageType]]
    sequence_promise: MessageSequencePromise

    def __init__(
        self,
        appender_capture_errors: Union[bool, Sentinel] = DEFAULT,
        start_asap: Union[bool, Sentinel] = DEFAULT,
        incoming_streamer: Optional[PromiseStreamer[MessageType]] = None,
    ) -> None:
        if incoming_streamer:
            # an external streamer is provided, so we don't create the default StreamAppender
            self.message_appender = None
        else:
            self.message_appender = StreamAppender(capture_errors=appender_capture_errors)
            incoming_streamer = self.message_appender

        super().__init__(
            incoming_streamer=incoming_streamer,
            start_asap=start_asap,
            sequence_promise_class=MessageSequencePromise,
        )

    @classmethod
    def turn_into_sequence_promise(cls, messages: MessageType) -> MessageSequencePromise:
        """
        Convert an arbitrarily nested collection of messages of various types (strings, dicts, Message objects,
        MessagePromise objects etc. - see `MessageType` definition for details) into a flat and uniform
        MessageSequencePromise object.
        """
        message_sequence = cls(
            appender_capture_errors=True,
            start_asap=False,
        )
        with message_sequence.message_appender:
            message_sequence.message_appender.append(messages)
        return message_sequence.sequence_promise

    @classmethod
    async def aresolve_messages(cls, messages: MessageType) -> tuple[Message, ...]:
        """
        Convert an arbitrarily nested collection of messages of various types (strings, dicts, Message objects,
        MessagePromise objects etc. - see `MessageType` definition for details) into a flat and uniform tuple of
        Message objects.
        """
        return await cls.turn_into_sequence_promise(messages).aresolve_messages()

    async def _flattener(  # pylint: disable=invalid-overridden-method
        self, zero_or_more_items: MessageType
    ) -> AsyncIterator[MessagePromise]:
        if isinstance(zero_or_more_items, MessagePromise):
            yield zero_or_more_items
        elif isinstance(zero_or_more_items, Message):
            yield zero_or_more_items.as_promise
        elif isinstance(zero_or_more_items, BaseModel):
            yield Message(**zero_or_more_items.model_dump()).as_promise
        elif isinstance(zero_or_more_items, dict):
            yield Message(**zero_or_more_items).as_promise
        elif isinstance(zero_or_more_items, str):
            yield Message(text=zero_or_more_items).as_promise
        elif isinstance(zero_or_more_items, BaseException):
            raise zero_or_more_items
        elif hasattr(zero_or_more_items, "__iter__"):
            for item in zero_or_more_items:
                async for message_promise in self._flattener(item):
                    yield message_promise
        elif hasattr(zero_or_more_items, "__aiter__"):
            async for item in zero_or_more_items:
                async for message_promise in self._flattener(item):
                    yield message_promise
        else:
            raise TypeError(f"Unexpected message type: {type(zero_or_more_items)}")


# noinspection PyProtectedMember
class AgentReplyMessageSequence(MessageSequence):
    # pylint: disable=protected-access
    """
    TODO Oleksandr: docstring
    """

    def __init__(
        self,
        mini_agent: MiniAgent,
        input_sequence_promise: MessageSequencePromise,
        function_kwargs: dict[str, Any],
        **kwargs,
    ) -> None:
        # this validates the agent function kwargs
        self._frozen_func_kwargs = Frozen(**function_kwargs).frozen_fields_and_values()
        self._function_kwargs = copy.deepcopy(function_kwargs)

        self._mini_agent = mini_agent
        self._input_sequence_promise = input_sequence_promise
        super().__init__(
            appender_capture_errors=True,  # we want `self.message_appender` not to let errors out of `run_the_agent`
            **kwargs,
        )

    async def _streamer(self, _) -> AsyncIterator[MessagePromise]:
        async def run_the_agent(_) -> AgentCallNode:
            ctx = InteractionContext(
                this_agent=self._mini_agent,
                messages=self._input_sequence_promise,
                reply_streamer=self.message_appender,
            )
            with self.message_appender:
                # errors are not raised above this `with` block, thanks to `appender_capture_errors=True`
                # pylint: disable=protected-access
                # noinspection PyProtectedMember
                await self._mini_agent._func(ctx, **self._function_kwargs)

            return AgentCallNode(
                messages=await self._input_sequence_promise.aresolve_messages(),
                agent_alias=self._mini_agent.alias,
                **self._mini_agent._interact_metadata_dict,
                # NOTE: the next line will override any keys from `self.interaction_metadata` if names collide
                **self._frozen_func_kwargs,
            )

        agent_call_promise = Promise[AgentCallNode](
            start_asap=True,
            resolver=run_the_agent,
        )

        async for reply_promise in super()._streamer(_):
            yield reply_promise  # at this point all MessageType items are "flattened" into MessagePromise items

        async def create_agent_reply_node(_) -> AgentReplyNode:
            return AgentReplyNode(
                replies=await self.sequence_promise.aresolve_messages(),
                agent_alias=self._mini_agent.alias,
                agent_call=await agent_call_promise,
                **self._mini_agent._interact_metadata_dict,
            )

        Promise[AgentReplyNode](
            start_asap=True,  # use a separate async task to avoid deadlock upon AgentReplyNode resolution
            resolver=create_agent_reply_node,
        )
```



miniagents/promising/errors.py
```python
"""
This module contains the custom errors used in the `Promising` part of the library.
"""


class PromisingError(Exception):
    """
    Base class for errors in the `Promising` part of the library.
    """


class FunctionNotProvidedError(PromisingError):
    """
    Raised when a function that was supposed to be provided (most likely as a parameter to a class constructor)
    was not provided.
    """


class AppenderNotOpenError(PromisingError):
    """
    Raised when an `StreamAppender` is not open for appending yet and an attempt is made to append to it.
    """


class AppenderClosedError(PromisingError):
    """
    Raised when an `StreamAppender` has already been closed for appending and an attempt is made to append to it.
    """
```



miniagents/promising/ext/frozen.py
```python
"""
The main class in this module is `Frozen`. See its docstring for more information.
"""

import hashlib
import itertools
import json
from functools import cached_property
from typing import Any, Iterator, Optional, Union

from pydantic import BaseModel, ConfigDict, model_validator

FrozenType = Optional[Union[str, int, float, bool, tuple["FrozenType", ...], "Frozen"]]


class Frozen(BaseModel):
    """
    A frozen pydantic model that allows arbitrary fields, has a git-style hash key that is calculated from the
    JSON representation of its data. The data is recursively validated to be immutable. Dicts are converted to
    `Frozen` instances, lists and tuples are converted to tuples of immutable values, sets are prohibited.
    """

    model_config = ConfigDict(frozen=True, extra="allow")

    class_: str

    def __str__(self) -> str:
        return self.as_string

    @cached_property
    def as_string(self) -> str:
        """
        Return a string representation of this model. This is usually the representation that will be used when
        the model needs to be a part of an LLM prompts.
        """
        # NOTE: child classes should override the private version, `_as_string()` if they want to customize behaviour
        return self._as_string()

    @cached_property
    def full_json(self) -> str:
        """
        Get the full JSON representation of this Frozen object together with all its nested objects. This is a cached
        property, so it is calculated only the first time it is accessed.
        """
        return self.model_dump_json()

    @cached_property
    def serialized(self) -> str:
        """
        The representation of this Frozen object that you would usually get by calling `serialize()`, but as a string
        with a JSON. This is a cached property, so it is calculated only the first time it is accessed.
        """
        return json.dumps(self.serialize(), ensure_ascii=False, sort_keys=True)

    def serialize(self) -> dict[str, Any]:
        """
        Serialize the object into a dictionary. The default implementation does complete serialization of this
        Frozen object and all its nested objects. Child classes may override this method to customize serialization
        (e.g. externalize certain nested objects and only reference them by their hash keys - see Message).
        """
        return self.model_dump()

    @cached_property
    def hash_key(self) -> str:
        """
        Get the hash key for this object. It is a hash of the JSON representation of the object.
        """
        # pylint: disable=cyclic-import,import-outside-toplevel
        from miniagents.promising.promising import PromisingContext

        hash_key = hashlib.sha256(self.serialized.encode("utf-8")).hexdigest()
        if not PromisingContext.get_current().longer_hash_keys:
            hash_key = hash_key[:40]
        return hash_key

    def frozen_fields(self, exclude_class: bool = False) -> Iterator[str]:
        """
        Get the list of field names of the object. This includes the model fields (both, explicitly set and the ones
        with default values) and the extra fields that are not part of the model.
        """
        if exclude_class:
            return itertools.chain(
                (field for field in self.model_fields if field != "class_"), self.__pydantic_extra__
            )
        return itertools.chain(self.model_fields, self.__pydantic_extra__)

    def frozen_fields_and_values(self, exclude_class: bool = True) -> dict[str, Any]:
        """
        Get a dict of field names and values of this Pydantic object. This includes the model fields (both,
        explicitly set and the ones with default values) and the extra fields that are not part of the model.
        """
        return dict(self._frozen_fields_and_values(exclude_class=exclude_class))

    def _frozen_fields_and_values(self, exclude_class: bool) -> Iterator[tuple[str, Any]]:
        if exclude_class:
            for field in self.model_fields:
                if field != "class_":
                    yield field, getattr(self, field)
        else:
            for field in self.model_fields:
                yield field, getattr(self, field)

        for field, value in self.__pydantic_extra__.items():  # pylint: disable=no-member
            yield field, value

    def _as_string(self) -> str:
        """
        Return the message as a string. This is the method that child classes should override to customize the string
        representation of the message for the LLM prompts.
        """
        return self.full_json

    @classmethod
    def _preprocess_values(cls, values: dict[str, Any]) -> dict[str, Any]:
        """
        Preprocess the values before validation and freezing.
        """
        # TODO Oleksandr: what about saving fully qualified model name, and not just the short name ?
        if "class_" in values:
            if values["class_"] != cls.__name__:
                raise ValueError(
                    f"the `class_` field of a Frozen must be equal to its actual class name, got {values['class_']} "
                    f"instead of {cls.__name__}"
                )
        else:
            values = {"class_": cls.__name__, **values}
        return values

    # noinspection PyNestedDecorators
    @model_validator(mode="before")
    @classmethod
    def _validate_and_freeze_values(cls, values: dict[str, Any]) -> dict[str, FrozenType]:
        """
        Recursively make sure that the field values of the object are immutable and of allowed types.
        """
        values = cls._preprocess_values(values)
        return {key: cls._validate_and_freeze_value(key, value) for key, value in values.items()}

    @classmethod
    def _validate_and_freeze_value(cls, key: str, value: Any) -> FrozenType:
        """
        Recursively make sure that the field value is immutable and of allowed type.
        """
        if isinstance(value, (tuple, list)):
            return tuple(cls._validate_and_freeze_value(key, sub_value) for sub_value in value)
        if isinstance(value, dict):
            return Frozen(**value)
        if not isinstance(value, cls._allowed_value_types()):
            raise ValueError(
                f"only {{{', '.join([t.__name__ for t in cls._allowed_value_types()])}}} "
                f"are allowed as field values in {cls.__name__}, got {type(value).__name__} in `{key}`"
            )
        return value

    @classmethod
    def _allowed_value_types(cls) -> tuple[type[Any], ...]:
        return type(None), str, int, float, bool, tuple, list, dict, Frozen
```



miniagents/promising/promise_typing.py
```python
"""
Types of the Promising part of the library.
"""

from typing import TypeVar, AsyncIterator, Protocol, Union, Any

T = TypeVar("T")
PIECE = TypeVar("PIECE")
WHOLE = TypeVar("WHOLE")
IN = TypeVar("IN")
OUT = TypeVar("OUT")
PromiseBound = TypeVar("PromiseBound", bound="Promise")
StreamedPromiseBound = TypeVar("StreamedPromiseBound", bound="StreamedPromise")
FlatSequenceBound = TypeVar("FlatSequenceBound", bound="FlatSequence")


class PromiseResolver(Protocol[T]):
    """
    TODO Oleksandr: docstring
    """

    async def __call__(self, promise: PromiseBound) -> T: ...


class PromiseStreamer(Protocol[PIECE]):
    """
    TODO Oleksandr: docstring
    """

    def __call__(self, streamed_promise: StreamedPromiseBound) -> AsyncIterator[PIECE]: ...


class PromiseResolvedEventHandler(Protocol):
    """
    A protocol for Promise resolution event handlers. A promise resolution event handler is a function that is
    scheduled to be called after Promise.aresolve() finishes resolving the promise. "Scheduled" means that the
    function is passed to the async event loop for execution without blocking the current coroutine.
    """

    async def __call__(self, promise: PromiseBound, result: Any) -> None: ...


class SequenceFlattener(Protocol[IN, OUT]):
    """
    A protocol for sequence flatteners. A sequence flattener is a function that takes a single object of type `IN`
    and asynchronously converts it into zero or more objects of type `OUT`. In other words, it "flattens" a single
    `IN` into zero or more instances of `OUT`.
    """

    def __call__(
        self, flat_sequence: FlatSequenceBound, zero_or_more_items: Union[IN, BaseException]
    ) -> AsyncIterator[OUT]: ...
```



miniagents/promising/promising.py
```python
"""
The main class in this module is `StreamedPromise`. See its docstring for more information.
"""

import asyncio
import contextvars
import logging
from asyncio import Task
from contextvars import ContextVar
from functools import partial
from types import TracebackType
from typing import Generic, AsyncIterator, Union, Optional, Iterable, Awaitable, Any

from miniagents.promising.errors import AppenderClosedError, AppenderNotOpenError, FunctionNotProvidedError
from miniagents.promising.promise_typing import (
    T,
    PIECE,
    WHOLE,
    PromiseStreamer,
    PromiseResolvedEventHandler,
    PromiseResolver,
)
from miniagents.promising.sentinels import Sentinel, NO_VALUE, FAILED, END_OF_QUEUE, DEFAULT

logger = logging.getLogger(__name__)


class PromisingContext:
    """
    This is the main class for managing the context of promises. It is a context manager that is used to configure
    default settings for promises and to handle the lifecycle of promises (attach `on_promise_resolved` handlers,
    ensure that all the async tasks finish before this context manager exits).
    """

    _current: ContextVar[Optional["PromisingContext"]] = ContextVar("PromisingContext._current", default=None)

    def __init__(
        self,
        start_everything_asap_by_default: bool = True,
        appenders_capture_errors_by_default: bool = False,
        longer_hash_keys: bool = False,
        log_level_for_errors: int = logging.ERROR,
        on_promise_resolved: Union[PromiseResolvedEventHandler, Iterable[PromiseResolvedEventHandler]] = (),
    ) -> None:
        self.parent = self._current.get()

        self.on_promise_resolved_handlers: list[PromiseResolvedEventHandler] = (
            [on_promise_resolved] if callable(on_promise_resolved) else [*on_promise_resolved]
        )
        self.child_tasks: set[Task] = set()

        self.start_everything_asap_by_default = start_everything_asap_by_default
        self.appenders_capture_errors_by_default = appenders_capture_errors_by_default
        self.longer_hash_keys = longer_hash_keys
        self.log_level_for_errors = log_level_for_errors

        self._previous_ctx_token: Optional[contextvars.Token] = None

    @classmethod
    def get_current(cls) -> "PromisingContext":
        """
        Get the current context. If no context is currently active, raise an error.
        """
        current = cls._current.get()
        if not current:
            raise RuntimeError(
                f"No {cls.__name__} is currently active. Did you forget to do `async with {cls.__name__}():`?"
            )
        if not isinstance(current, cls):
            raise TypeError(
                f"You seem to have done `async with {type(current).__name__}():` (or similar), "
                f"but `async with {cls.__name__}():` is expected instead."
            )
        return current

    def on_promise_resolved(self, handler: PromiseResolvedEventHandler) -> PromiseResolvedEventHandler:
        """
        Add a handler to be called after a promise is resolved.
        """
        self.on_promise_resolved_handlers.append(handler)
        return handler

    def start_asap(
        self,
        awaitable: Awaitable,
        suppress_errors: bool = False,
        log_level_for_errors: int = logging.DEBUG,
    ) -> Task:
        """
        Schedule a task in the current context. "Scheduling" a task this way instead of just creating it with
        `asyncio.create_task()` allows the context to keep track of the child tasks and to wait for them to finish
        before finalizing the context.
        """

        async def awaitable_wrapper() -> Any:
            # pylint: disable=broad-except
            # noinspection PyBroadException
            try:
                return await awaitable
            except Exception:
                logger.log(
                    log_level_for_errors,
                    "AN ERROR OCCURRED IN AN ASYNC BACKGROUND TASK",
                    exc_info=True,
                )
                if not suppress_errors:
                    raise
            except BaseException:
                if not suppress_errors:
                    raise
            finally:
                self.child_tasks.remove(task)

        task = asyncio.create_task(awaitable_wrapper())
        self.child_tasks.add(task)
        return task

    def activate(self) -> "PromisingContext":
        """
        Activate the context. This is a context manager method that is used to activate the context for the duration
        of the `async with` block. Can be called as a regular method as well in cases where it is not possible to use
        the `async with` block (e.g., if a PromisingContext needs to be activated for the duration of an async webserver
        being up).
        """
        if self._previous_ctx_token:
            raise RuntimeError("PromisingContext is not reentrant")
        self._previous_ctx_token = self._current.set(self)  # <- this is the context switch
        return self

    async def aflush_tasks(self) -> None:
        """
        Wait for all the child tasks to finish. This is useful when you want to wait for all the child tasks to finish
        before proceeding with the rest of the code.
        """
        while self.child_tasks:
            await asyncio.gather(
                *self.child_tasks,
                return_exceptions=True,  # this prevents waiting until the first exception and then giving up
            )

    async def afinalize(self) -> None:
        """
        Finalize the context (wait for all the child tasks to finish and reset the context). This method is called
        automatically at the end of the `async with` block.
        """
        await self.aflush_tasks()
        self._current.reset(self._previous_ctx_token)
        self._previous_ctx_token = None

    async def __aenter__(self) -> "PromisingContext":
        return self.activate()

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        await self.afinalize()


class Promise(Generic[T]):
    """
    TODO Oleksandr: docstring
    """

    def __init__(
        self,
        start_asap: Union[bool, Sentinel] = DEFAULT,
        resolver: Optional[PromiseResolver[T]] = None,
        prefill_result: Union[Optional[T], Sentinel] = NO_VALUE,
    ) -> None:
        # TODO Oleksandr: raise an error if both prefill_result and resolver are set (or both are not set)
        promising_context = PromisingContext.get_current()

        if start_asap is DEFAULT:
            start_asap = promising_context.start_everything_asap_by_default

        if resolver:
            self._resolver = partial(resolver, self)

        if prefill_result is NO_VALUE:
            # NO_VALUE is used because `None` is also a legitimate value
            self._result: Union[T, Sentinel, BaseException] = NO_VALUE
        else:
            self._result = prefill_result
            self._trigger_promise_resolved_event()

        self._resolver_lock = asyncio.Lock()

        if start_asap and prefill_result is NO_VALUE:
            promising_context.start_asap(
                self, suppress_errors=True, log_level_for_errors=promising_context.log_level_for_errors
            )

    async def _resolver(self) -> T:  # pylint: disable=method-hidden
        raise FunctionNotProvidedError(
            "The `resolver` function should be provided either via the constructor "
            "or by subclassing the `Promise` class."
        )

    async def aresolve(self) -> T:
        """
        TODO Oleksandr: docstring
        """
        # TODO Oleksandr: put a deadlock prevention mechanism in place, i. e. find a way to disallow calling
        #  `aresolve()` from within the `resolver` function
        if self._result is NO_VALUE:
            async with self._resolver_lock:
                if self._result is NO_VALUE:
                    try:
                        self._result = await self._resolver()
                    except BaseException as exc:  # pylint: disable=broad-except
                        logger.debug("An error occurred while resolving a Promise", exc_info=True)
                        self._result = exc

                    self._trigger_promise_resolved_event()

        if isinstance(self._result, BaseException):
            raise self._result
        return self._result

    def __await__(self):
        return self.aresolve().__await__()

    def _trigger_promise_resolved_event(self):
        promising_context = PromisingContext.get_current()
        while promising_context:
            for handler in promising_context.on_promise_resolved_handlers:
                promising_context.start_asap(
                    handler(self, self._result),
                    suppress_errors=True,
                    log_level_for_errors=promising_context.log_level_for_errors,
                )
            promising_context = promising_context.parent


class StreamedPromise(Generic[PIECE, WHOLE], Promise[WHOLE]):
    """
    A StreamedPromise represents a promise of a whole value that can be streamed piece by piece.

    The StreamedPromise allows for "replaying" the stream of pieces without involving the `streamer`
    function for the pieces that have already been produced. This means that multiple consumers can
    iterate over the stream independently, and each consumer will receive all the pieces from the
    beginning, even if some pieces were produced before the consumer started iterating over the
    promise.

    :param streamer: A callable that returns an async iterator yielding the pieces of the whole value.
    :param resolver: A callable that takes an async iterable of pieces and returns the whole value
                     ("packages" the pieces).
    TODO Oleksandr: explain the `start_asap` parameter
    TODO Oleksandr: this is one of the central classes of the framework, hence the docstring should be
     much more detailed
    """

    def __init__(
        self,
        streamer: Optional[PromiseStreamer[PIECE]] = None,
        prefill_pieces: Union[Optional[Iterable[PIECE]], Sentinel] = NO_VALUE,
        resolver: Optional[PromiseResolver[T]] = None,
        prefill_result: Union[Optional[T], Sentinel] = NO_VALUE,
        start_asap: Union[bool, Sentinel] = DEFAULT,
    ) -> None:
        # TODO Oleksandr: raise an error if both prefill_pieces and streamer are set (or both are not set)
        promising_context = PromisingContext.get_current()

        if start_asap is DEFAULT:
            start_asap = promising_context.start_everything_asap_by_default

        super().__init__(
            start_asap=start_asap,
            resolver=resolver,
            prefill_result=prefill_result,
        )

        if streamer:
            self._streamer = partial(streamer, self)

        if prefill_pieces is NO_VALUE:
            self._pieces_so_far: list[Union[PIECE, BaseException]] = []
        else:
            self._pieces_so_far: list[Union[PIECE, BaseException]] = [*prefill_pieces, StopAsyncIteration()]

        self._all_pieces_consumed = prefill_pieces is not NO_VALUE
        self._streamer_lock = asyncio.Lock()

        if start_asap and prefill_pieces is NO_VALUE:
            # start producing pieces at the earliest task switch (put them in a queue for further consumption)
            self._queue = asyncio.Queue()
            promising_context.start_asap(
                self._aconsume_the_stream(),
                suppress_errors=True,
                log_level_for_errors=promising_context.log_level_for_errors,
            )
        else:
            # each piece will be produced on demand (when the first consumer iterates over it and not earlier)
            self._queue = None

        self._streamer_aiter: Union[Optional[AsyncIterator[PIECE]], Sentinel] = None

    def _streamer(self) -> AsyncIterator[PIECE]:  # pylint: disable=method-hidden
        raise FunctionNotProvidedError(
            "The `streamer` function should be provided either via the constructor "
            "or by subclassing the `StreamedPromise` class."
        )

    def __aiter__(self) -> AsyncIterator[PIECE]:
        """
        This allows to consume the stream piece by piece. Each new iterator returned by `__aiter__` will replay
        the stream from the beginning.
        """
        return self._StreamReplayIterator(self)

    def __call__(self, *args, **kwargs) -> AsyncIterator[PIECE]:
        """
        This enables the `StreamedPromise` to be used as a piece streamer for another `StreamedPromise`, effectively
        chaining them together.
        """
        return self.__aiter__()

    async def _aconsume_the_stream(self) -> None:
        while True:
            piece = await self._streamer_aiter_anext()
            self._queue.put_nowait(piece)
            if isinstance(piece, StopAsyncIteration):
                break

    async def _streamer_aiter_anext(self) -> Union[PIECE, BaseException]:
        # pylint: disable=broad-except
        if self._streamer_aiter is None:
            try:
                self._streamer_aiter = self._streamer()
                # noinspection PyUnresolvedReferences
                if not callable(self._streamer_aiter.__anext__):
                    raise TypeError("The streamer must return an async iterator")
            except BaseException as exc:
                logger.debug("An error occurred while instantiating a streamer for a StreamedPromise", exc_info=True)
                self._streamer_aiter = FAILED
                return exc

        elif self._streamer_aiter is FAILED:
            # we were not able to instantiate the streamer iterator at all - stopping the stream
            return StopAsyncIteration()

        try:
            return await self._streamer_aiter.__anext__()
        except BaseException as exc:
            if not isinstance(exc, StopAsyncIteration):
                logger.debug(
                    'An error occurred while fetching a single "piece" of a StreamedPromise from its pieces streamer.',
                    exc_info=True,
                )
            # Any exception, apart from `StopAsyncIteration`, will always be stored in the `_pieces_so_far` list
            # before the `StopAsyncIteration` and will not conclude the list (in other words, `StopAsyncIteration`
            # will always conclude the `_pieces_so_far` list). This is because if you keep iterating over an
            # iterator/generator past any other exception that it might raise, it is still supposed to raise
            # `StopAsyncIteration` at the end.
            return exc

    class _StreamReplayIterator(AsyncIterator[PIECE]):
        """
        The pieces that have already been "produced" are stored in the `_pieces_so_far` attribute of the parent
        `StreamedPromise`. The `_StreamReplayIterator` first yields the pieces from `_pieces_so_far`, and then it
        continues to retrieve new pieces from the original streamer of the parent `StreamedPromise`
        (`_streamer_aiter` attribute of the parent `StreamedPromise`).
        """

        def __init__(self, streamed_promise: "StreamedPromise") -> None:
            self._streamed_promise = streamed_promise
            self._index = 0

        async def __anext__(self) -> PIECE:
            if self._index < len(self._streamed_promise._pieces_so_far):
                # "replay" a piece that was produced earlier
                piece = self._streamed_promise._pieces_so_far[self._index]
            elif self._streamed_promise._all_pieces_consumed:
                # we know that `StopAsyncIteration` was stored as the last piece in the piece list
                raise self._streamed_promise._pieces_so_far[-1]
            else:
                async with self._streamed_promise._streamer_lock:
                    if self._index < len(self._streamed_promise._pieces_so_far):
                        piece = self._streamed_promise._pieces_so_far[self._index]
                    else:
                        piece = await self._real_anext()

            self._index += 1

            if isinstance(piece, BaseException):
                raise piece
            return piece

        async def _real_anext(self) -> Union[PIECE, BaseException]:
            # pylint: disable=protected-access
            if self._streamed_promise._queue is None:
                # the stream is being produced on demand, not beforehand (`start_asap` is False)
                piece = await self._streamed_promise._streamer_aiter_anext()
            else:
                # the stream is being produced beforehand (`start_asap` is True)
                piece = await self._streamed_promise._queue.get()

            if isinstance(piece, StopAsyncIteration):
                # `StopAsyncIteration` will be stored as the last piece in the piece list
                self._streamed_promise._all_pieces_consumed = True

            self._streamed_promise._pieces_so_far.append(piece)
            return piece


class StreamAppender(Generic[PIECE], AsyncIterator[PIECE]):
    """
    This is a special kind of `streamer` that can be fed into `StreamedPromise` constructor. Objects of this class
    implement the context manager protocol and an `append()` method, which allows for passing such an object into
    `StreamedPromise` constructor while also keeping a reference to it in the outside code in order to `feed` the
    pieces into it (and, consequently, into the `StreamedPromise`) later using `append()`.
    TODO Oleksandr: explain the `capture_errors` parameter
    """

    def __init__(self, capture_errors: Union[bool, Sentinel] = DEFAULT) -> None:
        self._queue = asyncio.Queue()
        self._append_open = False
        self._append_closed = False
        if capture_errors is DEFAULT:
            self._capture_errors = PromisingContext.get_current().appenders_capture_errors_by_default
        else:
            self._capture_errors = capture_errors

    def __enter__(self) -> "StreamAppender":
        return self.open()

    def __exit__(
        self,
        exc_type: Optional[type[BaseException]],
        exc_value: Optional[BaseException],
        traceback: Optional[TracebackType],
    ) -> bool:
        is_append_closed_error = isinstance(exc_value, AppenderClosedError)
        error_should_not_propagate = self._capture_errors and not is_append_closed_error

        if exc_value and error_should_not_propagate:
            logger.debug("An error occurred while appending pieces to a StreamAppender", exc_info=exc_value)
            self.append(exc_value)
        self.close()

        # if `capture_errors` is True, then we also return True, so that the exception is not propagated outside
        # the `with` block (except if the error is an `AppenderClosedError` - in this case, we do not suppress it)
        return error_should_not_propagate

    def append(self, piece: PIECE) -> "StreamAppender":
        """
        Append a `piece` to the streamer. This method can only be called when the streamer is open for appending (and
        also not closed yet). Consequently, the `piece` is delivered to the `StreamedPromise` that is consuming from
        this streamer.
        """
        if not self._append_open:
            raise AppenderNotOpenError(
                "You need to put the `append()` operation inside a `with StreamAppender()` block "
                "(or call `open()` and `close()` manually)."
            )
        if self._append_closed:
            raise AppenderClosedError("The StreamAppender has already been closed for appending.")
        self._queue.put_nowait(piece)
        return self

    def open(self) -> "StreamAppender":
        """
        Open the streamer for appending.

        ATTENTION! It is highly recommended to use the `with` statement instead of calling `open()` and `close()`
        manually.

        Forgetting to call `close()` or not calling it due to an exception will result in `StreamedPromise`
        (and the code that is consuming from it) waiting for more `pieces` forever.
        """
        if self._append_closed:
            raise AppenderClosedError("Once closed, the StreamAppender cannot be opened again.")
        self._append_open = True
        return self

    def close(self) -> None:
        """
        Close the streamer after all the pieces have been appended.

        ATTENTION! It is highly recommended to use the `with` statement instead of calling `open()` and `close()`
        manually.

        Forgetting to call `close()` or not calling it due to an exception will result in `StreamedPromise`
        (and the code that is consuming from it) waiting for more `pieces` forever.
        """
        if self._append_closed:
            return
        self._append_closed = True
        self._queue.put_nowait(END_OF_QUEUE)

    async def __anext__(self) -> PIECE:
        if self._queue is None:
            raise StopAsyncIteration()

        piece = await self._queue.get()
        if piece is END_OF_QUEUE:
            self._queue = None
            raise StopAsyncIteration()

        return piece

    def __call__(self, *args, **kwargs) -> AsyncIterator[PIECE]:
        return self
```



miniagents/promising/sentinels.py
```python
"""
See the docstring of the `Sentinel` class for more information.
"""


class Sentinel:
    """
    A sentinel object that is used indicate things like NO_VALUE (when None is considered a value), DEFAULT, etc.
    """

    def __bool__(self) -> bool:
        raise RuntimeError("Sentinels should not be used in boolean expressions.")


NO_VALUE = Sentinel()
DEFAULT = Sentinel()
FAILED = Sentinel()
END_OF_QUEUE = Sentinel()
AWAIT = Sentinel()
CLEAR = Sentinel()
```



miniagents/promising/sequence.py
```python
"""
The main class in this module is `FlatSequence`. See its docstring for more information.
"""

from functools import partial
from typing import Generic, AsyncIterator, Union, Optional

from miniagents.promising.errors import FunctionNotProvidedError
from miniagents.promising.promise_typing import SequenceFlattener, IN, OUT, PromiseStreamer
from miniagents.promising.promising import StreamedPromise
from miniagents.promising.sentinels import Sentinel, DEFAULT


class FlatSequence(Generic[IN, OUT]):
    """
    TODO Oleksandr: docstring
    """

    sequence_promise: StreamedPromise[OUT, tuple[OUT, ...]]

    def __init__(
        self,
        incoming_streamer: PromiseStreamer[IN],
        flattener: Optional[SequenceFlattener[IN, OUT]] = None,
        start_asap: Union[bool, Sentinel] = DEFAULT,
        sequence_promise_class: type[StreamedPromise[OUT, tuple[OUT, ...]]] = StreamedPromise[OUT, tuple[OUT, ...]],
    ) -> None:
        if flattener:
            self._flattener = partial(flattener, self)

        self._input_promise = StreamedPromise(
            streamer=self._streamer,
            resolver=lambda _: None,
            start_asap=False,
        )
        # TODO Oleksandr: should I really pass `self` here ? it is not of type `StreamedPromiseBound`
        self._incoming_streamer_aiter = incoming_streamer(self)

        self.sequence_promise = sequence_promise_class(
            streamer=self._input_promise,
            resolver=self._resolver,
            start_asap=start_asap,
        )

    def _flattener(self, zero_or_more_items: IN) -> AsyncIterator[OUT]:  # pylint: disable=method-hidden
        # TODO Oleksandr: come up with a different method name ?
        raise FunctionNotProvidedError(
            "The `flattener` function should be provided either via the constructor "
            "or by subclassing the `FlatSequence` class."
        )

    async def _streamer(self, _) -> AsyncIterator[OUT]:
        async for zero_or_more_items in self._incoming_streamer_aiter:
            async for item in self._flattener(zero_or_more_items):
                yield item

    async def _resolver(self, _) -> tuple[OUT, ...]:
        return tuple([item async for item in self.sequence_promise])  # pylint: disable=consider-using-generator
```



miniagents/utils.py
```python
"""
Utility functions of the MiniAgents framework.
"""

import logging
from typing import AsyncIterator, Any, Optional, Union, Iterable, Callable

from pydantic._internal._model_construction import ModelMetaclass

from miniagents.messages import MessageSequencePromise
from miniagents.miniagents import MessageType, MessageSequence, MessagePromise, Message, MiniAgent
from miniagents.promising.promising import StreamAppender
from miniagents.promising.sentinels import Sentinel, DEFAULT, AWAIT, CLEAR

logger = logging.getLogger(__name__)


async def adialog_loop(
    user_agent: Union[MiniAgent, Callable[[MessageType, ...], MessageSequencePromise], Sentinel],
    assistant_agent: Union[MiniAgent, Callable[[MessageType, ...], MessageSequencePromise], Sentinel],
) -> None:
    """
    Run a loop that chains the user agent and the assistant agent in a dialog.
    """
    await achain_loop(
        [
            user_agent,
            AWAIT,  # TODO Oleksandr: explain this with an inline comment like this one
            assistant_agent,
        ]
    )


async def achain_loop(
    agents: Iterable[Union[MiniAgent, Callable[[MessageType, ...], MessageSequencePromise], Sentinel]],
    initial_input: Optional[MessageType] = None,
) -> None:
    """
    Run a loop that chains the agents together.
    """
    agents = list(agents)
    if not any(agent is AWAIT for agent in agents):
        raise ValueError(
            "There should be at least one AWAIT sentinel in the list of agents in order for the loop not to "
            "schedule the turns infinitely without actually running them."
        )

    messages = initial_input
    while True:
        for agent in agents:
            if agent is AWAIT:
                if isinstance(messages, MessageSequencePromise):
                    # all the interactions happen here (here all the scheduled promises are awaited for)
                    messages = await messages.aresolve_messages()
            elif agent is CLEAR:
                messages = None
            elif callable(agent):
                messages = agent(messages)
            elif isinstance(agent, MiniAgent):
                messages = agent.inquire(messages)
            else:
                raise ValueError(f"Invalid agent: {agent}")
    # TODO Oleksandr: How should agents end the loop ? What message sequence should this utility return when the
    #  loop is over ?


def join_messages(
    messages: MessageType,
    delimiter: Optional[str] = "\n\n",
    strip_leading_newlines: bool = False,
    reference_original_messages: bool = True,
    start_asap: Union[bool, Sentinel] = DEFAULT,  # TODO Oleksandr: why not just make it False ?
    **message_metadata,
) -> MessagePromise:
    """
    Join multiple messages into a single message using a delimiter.

    :param messages: Messages to join.
    :param delimiter: A string that will be inserted between messages.
    :param strip_leading_newlines: If True, leading newlines will be stripped from each message. Language models,
    when prompted in a certain way, may produce leading newlines in the response. This parameter allows you to
    remove them.
    :param reference_original_messages: If True, the resulting message will contain the list of original messages in
    the `original_messages` field.
    :param start_asap: If True, the resulting message will be scheduled for background resolution regardless
    of when it is going to be consumed.
    :param message_metadata: Additional metadata to be added to the resulting message.
    """

    async def token_streamer(metadata_so_far: dict[str, Any]) -> AsyncIterator[str]:
        metadata_so_far.update(message_metadata)
        if reference_original_messages:
            metadata_so_far["original_messages"] = []

        first_message = True
        async for message_promise in MessageSequence.turn_into_sequence_promise(messages):
            # TODO Oleksandr: accumulate metadata from all the messages !!!
            if delimiter and not first_message:
                yield delimiter

            lstrip_newlines = strip_leading_newlines
            async for token in message_promise:
                if lstrip_newlines:
                    # let's remove leading newlines from the first message
                    token = token.lstrip("\n\r")
                if token:
                    lstrip_newlines = False  # non-empty token was found - time to stop stripping newlines
                    yield token

            if reference_original_messages:
                metadata_so_far["original_messages"].append(await message_promise)

            first_message = False

    return Message.promise(
        message_token_streamer=token_streamer,
        start_asap=start_asap,
    )


def split_messages(  # TODO Oleksandr: move this function into some kind of `experimental` module ?
    messages: MessageType,
    delimiter: str = "\n\n",
    code_block_delimiter: Optional[str] = "```",
    start_asap: Union[bool, Sentinel] = DEFAULT,
    **message_metadata,
) -> MessageSequencePromise:
    """
    TODO Oleksandr: docstring
    """

    # pylint: disable=not-context-manager,too-many-statements

    # TODO Oleksandr: convert this function into a class ?
    # TODO Oleksandr: simplify this function somehow ? it is not going to be easy to understand later
    # TODO Oleksandr: but cover it with unit tests first
    async def sequence_streamer(_) -> AsyncIterator[MessagePromise]:
        text_so_far = ""
        current_text_appender: Optional[StreamAppender[str]] = None
        inside_code_block = False

        def is_text_so_far_not_empty() -> bool:
            return bool(text_so_far.replace(delimiter, ""))

        def split_text_if_needed() -> bool:
            nonlocal text_so_far, current_text_appender, inside_code_block

            delimiter_idx = -1 if inside_code_block else text_so_far.find(delimiter)
            delimiter_len = len(delimiter)

            code_delimiter_idx = text_so_far.find(
                code_block_delimiter,
                len(code_block_delimiter) if inside_code_block else 0,  # skip the opening delimiter if we're inside
            )
            if code_delimiter_idx > -1 and (delimiter_idx < 0 or code_delimiter_idx < delimiter_idx):
                delimiter_len = 0  # we want to include the code block delimiters into the text of the code message
                if inside_code_block:
                    delimiter_idx = code_delimiter_idx + len(code_block_delimiter)
                else:
                    delimiter_idx = code_delimiter_idx
                inside_code_block = not inside_code_block

            if delimiter_idx < 0:
                return False

            text = text_so_far[:delimiter_idx]
            text_so_far = text_so_far[delimiter_idx + delimiter_len :]
            if text:
                with current_text_appender:
                    current_text_appender.append(text)
                current_text_appender = None
            return True

        def start_new_message_promise() -> MessagePromise:
            nonlocal current_text_appender
            current_text_appender = StreamAppender[str]()

            async def token_streamer(metadata_so_far: dict[str, Any]) -> AsyncIterator[str]:
                metadata_so_far.update(message_metadata)
                async for token in current_text_appender:
                    yield token

            return Message.promise(
                message_token_streamer=token_streamer,
                start_asap=start_asap,
            )

        try:
            if not current_text_appender:
                # we already know that there will be at least one message - time to make a promise
                yield start_new_message_promise()

            async for token in join_messages(
                messages,
                delimiter=delimiter,
                reference_original_messages=False,
                start_asap=start_asap,
            ):
                text_so_far += token

                while True:
                    # TODO Oleksandr: this loop doesn't really work when we are not in streaming mode (when the whole
                    #  message is available at once) - only the first and the last paragraph is returned, middle
                    #  paragraphs are lost
                    if not current_text_appender and is_text_so_far_not_empty():
                        # previous message was already sent - we need to start a new one (make a new promise)
                        yield start_new_message_promise()
                    if not split_text_if_needed():
                        # repeat splitting until no more splitting is happening anymore in the text that we have so far
                        break

            if is_text_so_far_not_empty():
                # some text still remains after all the messages have been processed
                if current_text_appender:
                    with current_text_appender:
                        current_text_appender.append(text_so_far)
                else:
                    yield Message(text=text_so_far, **message_metadata).as_promise

        except BaseException as exc:  # pylint: disable=broad-except
            logger.debug("Error while processing a message sequence inside `split_messages`", exc_info=True)
            if current_text_appender:
                with current_text_appender:
                    # noinspection PyTypeChecker
                    current_text_appender.append(exc)  # TODO Oleksandr: update StreamAppender's signature ?
            else:
                raise exc
        finally:
            if current_text_appender:
                # in case of an exception and the last MessagePromise "still hanging"
                current_text_appender.close()

    async def sequence_resolver(sequence_promise: MessageSequencePromise) -> tuple[MessagePromise, ...]:
        return tuple([item async for item in sequence_promise])  # pylint: disable=consider-using-generator

    return MessageSequencePromise(
        streamer=sequence_streamer,
        resolver=sequence_resolver,
        start_asap=True,  # allowing it to ever be False results in a deadlock
    )


class SingletonMeta(type):
    """
    A metaclass that ensures that only one instance of a certain class is created.
    NOTE: This metaclass is designed to work in asynchronous environments, hence we didn't bother making
    it thread-safe (people typically don't mix multithreading and asynchronous paradigms together).
    """

    def __call__(cls):
        if not hasattr(cls, "_instance"):
            cls._instance = super().__call__()
        return cls._instance


class Singleton(metaclass=SingletonMeta):
    """
    A class that ensures that only one instance of a certain class is created.
    """


class ModelSingletonMeta(ModelMetaclass, SingletonMeta):
    """
    A metaclass that ensures that only one instance of a Pydantic model of a certain class is created.
    TODO Oleksandr: check if this class works at all
    """


class ModelSingleton(metaclass=ModelSingletonMeta):
    """
    A class that ensures that only one instance of a Pydantic model of a certain class is created.
    """
```



pyproject.toml
```
[tool.black]
line-length = 119

[tool.coverage.run]
branch = true

[tool.poetry]
name = "miniagents"
version = "0.0.14"
description = """\
An asynchronous framework for building LLM-based multi-agent systems in Python, with a focus on immutable messages \
and token streaming.\
"""
authors = ["Oleksandr Tereshchenko <toporok@gmail.com>"]
homepage = "https://github.com/teremterem/MiniAgents"
readme = "README.md"
license = "MIT"

[tool.poetry.dependencies]
python = ">=3.9,<4.0"
pydantic = ">=2.0.0,<3.0.0"

[tool.poetry.dev-dependencies]
anthropic = "*"
black = "*"
ipython = "*"
jupyterlab = "*"
markdown-it-py = "*"
notebook = "*"
openai = "*"
pre-commit = "*"
# promptlayer = "*"
pylint = "*"
pytest = "*"
pytest-asyncio = "*"
pytest-cov = "*"
python-dotenv = "*"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```



tests/test_agents.py
```python
"""
Test the agents.
"""

import asyncio
from typing import Union

import pytest

from miniagents.miniagents import MiniAgents, miniagent, InteractionContext
from miniagents.promising.sentinels import DEFAULT, Sentinel


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_agents_run_in_parallel(start_asap: Union[bool, Sentinel]) -> None:
    """
    Test that agents can run in parallel.
    """
    event_sequence = []

    @miniagent
    async def agent1(_) -> None:
        event_sequence.append("agent1 - start")
        await asyncio.sleep(0.1)
        event_sequence.append("agent1 - end")

    @miniagent
    async def agent2(_) -> None:
        event_sequence.append("agent2 - start")
        await asyncio.sleep(0.1)
        event_sequence.append("agent2 - end")

    async with MiniAgents():
        replies1 = agent1.inquire(start_asap=start_asap)
        replies2 = agent2.inquire(start_asap=start_asap)
        if start_asap is False:
            # when agents are not scheduled to start ASAP, their result needs to be awaited for explicitly in order
            # for their respective functions to be called
            await replies1.aresolve_messages()
            await replies2.aresolve_messages()

    if start_asap is DEFAULT or start_asap is True:
        # for MiniAgents() True is the DEFAULT
        assert event_sequence == [
            "agent1 - start",
            "agent2 - start",
            "agent1 - end",
            "agent2 - end",
        ]
    else:
        # if agents aren't scheduled to start ASAP, then they are processed in this test sequentially
        assert event_sequence == [
            "agent1 - start",
            "agent1 - end",
            "agent2 - start",
            "agent2 - end",
        ]


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_sub_agents_run_in_parallel(start_asap: Union[bool, Sentinel]) -> None:
    """
    Test that two agents that were called by the third agent can run in parallel.
    """
    event_sequence = []

    @miniagent
    async def agent1(_) -> None:
        event_sequence.append("agent1 - start")
        await asyncio.sleep(0.1)
        event_sequence.append("agent1 - end")

    @miniagent
    async def agent2(_) -> None:
        event_sequence.append("agent2 - start")
        await asyncio.sleep(0.1)
        event_sequence.append("agent2 - end")

    @miniagent
    async def aggregation_agent(ctx: InteractionContext) -> None:
        # wrapping this generator into a list comprehension is necessary to make sure that the agents are called
        # immediately (and are executed in parallel as a result)
        ctx.reply([agent.inquire(start_asap=start_asap) for agent in [agent1, agent2]])

    async with MiniAgents():
        replies = aggregation_agent.inquire(start_asap=start_asap)
        if start_asap is False:
            # when agents are not scheduled to start ASAP, their result needs to be awaited for explicitly in order
            # for their respective functions to be called
            await replies.aresolve_messages()

    if start_asap is DEFAULT or start_asap is True:
        # for MiniAgents() True is the DEFAULT
        assert event_sequence == [
            "agent1 - start",
            "agent2 - start",
            "agent1 - end",
            "agent2 - end",
        ]
    else:
        # if agents aren't scheduled to start ASAP, then they are processed in this test sequentially
        assert event_sequence == [
            "agent1 - start",
            "agent1 - end",
            "agent2 - start",
            "agent2 - end",
        ]
```



tests/test_frozen.py
```python
"""
Tests for the `Frozen`-based models.
"""

import hashlib
from typing import Optional
from unittest.mock import patch

import pytest
from pydantic import ValidationError

from miniagents.promising.ext.frozen import Frozen
from miniagents.promising.promising import PromisingContext


class SampleModel(Frozen):
    """
    A sample immutable subclass that is derived from `Frozen`.
    """

    some_req_field: str
    some_opt_field: int = 2
    sub_model: Optional["SampleModel"] = None


def test_sample_model_frozen() -> None:
    """
    Test that the models of `SampleModel`, which is derived from `Frozen`, are frozen.
    """
    sample = SampleModel(some_req_field="test")

    with pytest.raises(ValidationError):
        sample.some_req_field = "test2"
    with pytest.raises(ValidationError):
        sample.some_opt_field = 3

    assert sample.some_req_field == "test"
    assert sample.some_opt_field == 2


def test_model_frozen() -> None:
    """
    Test that the models of the original `Frozen` class are frozen.
    """
    model = Frozen(some_field="some value")

    with pytest.raises(ValidationError):
        model.some_other_field = "some other value"

    assert model.some_field == "some value"


@pytest.mark.asyncio
async def test_sample_model_hash_key() -> None:
    """
    Test `SampleModel.hash_key` property.
    """
    async with PromisingContext():
        sample = SampleModel(some_req_field="test", sub_model=SampleModel(some_req_field="юнікод", some_opt_field=3))
        # Let's make sure that private instance attributes that were not declared in the model beforehand:
        #  1) are settable despite the model being frozen;
        #  2) do not influence the hash_key.
        # MiniAgents.on_persist_message event sets a private attribute on Message instances, hence we want to
        # ensure these properties.
        # pylint: disable=protected-access,attribute-defined-outside-init
        sample._some_private_attribute = "some value"

        # print(json.dumps(sample.model_dump(), ensure_ascii=False, sort_keys=True))
        expected_hash_key = hashlib.sha256(
            '{"class_": "SampleModel", "some_opt_field": 2, "some_req_field": "test", "sub_model": '
            '{"class_": "SampleModel", "some_opt_field": 3, "some_req_field": "юнікод", "sub_model": null}}'
            "".encode("utf-8")
        ).hexdigest()[:40]
        assert sample.hash_key == expected_hash_key


@pytest.mark.asyncio
async def test_model_hash_key() -> None:
    """
    Test the original `Frozen.hash_key` property.
    """
    async with PromisingContext():
        model = Frozen(content="test", final_sender_alias="user", custom_field={"role": "user"})
        # print(json.dumps(model.model_dump(exclude={"forum_trees"}), ensure_ascii=False, sort_keys=True))
        expected_hash_key = hashlib.sha256(
            '{"class_": "Frozen", "content": "test", "custom_field": {"class_": "Frozen", "role": "user"}, '
            '"final_sender_alias": "user"}'.encode("utf-8")
        ).hexdigest()[:40]
        assert model.hash_key == expected_hash_key


def test_nested_object_not_copied() -> None:
    """
    Test that nested objects are not copied when the outer pydantic model is created.
    TODO Oleksandr: why do you care about this ?
    """
    sub_model = SampleModel(some_req_field="test")
    sample = SampleModel(some_req_field="test", sub_model=sub_model)

    assert sample.sub_model is sub_model


@pytest.mark.asyncio
async def test_hash_key_calculated_once() -> None:
    """
    Test that `SampleModel.hash_key` property is calculated only once and all subsequent calls return the same
    value without calculating it again.
    """
    original_sha256 = hashlib.sha256

    with patch("hashlib.sha256", side_effect=original_sha256) as mock_sha256:
        async with PromisingContext():
            sample = SampleModel(some_req_field="test")
            mock_sha256.assert_not_called()  # not calculated yet

            assert sample.hash_key == "2f9753c92f0452bacafaa606b6076d2bf266e095"
            mock_sha256.assert_called_once()  # calculated once

            assert sample.hash_key == "2f9753c92f0452bacafaa606b6076d2bf266e095"
            mock_sha256.assert_called_once()  # check that it wasn't calculated again


@pytest.mark.asyncio
async def test_model_hash_key_vs_key_ordering() -> None:
    """
    Test that `hash_key` of `Frozen` is not affected by the ordering of its fields.
    """
    async with PromisingContext():
        model1 = Frozen(some_field="test", some_other_field=2)
        model2 = Frozen(some_other_field=2, some_field="test")

        assert model1.hash_key == model2.hash_key
```



tests/test_llm.py
```python
# noinspection PyUnresolvedReferences
"""
Test the LLM agents.
"""

from typing import Callable

import pytest
from dotenv import load_dotenv

from miniagents.messages import Message
from miniagents.miniagents import MiniAgents, MiniAgent

load_dotenv()

# pylint: disable=wrong-import-position
from miniagents.ext.llm.anthropic import anthropic_agent
from miniagents.ext.llm.openai import openai_agent


def _check_openai_response(message: Message) -> None:
    assert message.text.strip() == "I AM ONLINE"
    assert message.choices[0].finish_reason == "stop"


def _check_anthropic_response(message: Message) -> None:
    assert message.text.strip() == "I AM ONLINE"
    assert message.stop_reason == "end_turn"


@pytest.mark.parametrize(
    "llm_agent, check_response_func",
    [
        (openai_agent.fork(model="gpt-3.5-turbo-0125"), _check_openai_response),
        (anthropic_agent.fork(model="claude-3-haiku-20240307"), _check_anthropic_response),
    ],
)
@pytest.mark.asyncio
@pytest.mark.parametrize("stream", [False, True])
@pytest.mark.parametrize("start_asap", [False, True])
async def test_llm(
    start_asap: bool,
    stream: bool,
    llm_agent: MiniAgent,
    check_response_func: Callable[[Message], None],
) -> None:
    """
    Assert that all the LLM agents can respond to a simple prompt.
    """
    async with MiniAgents(start_everything_asap_by_default=start_asap):
        reply_sequence = llm_agent.inquire(
            Message(text="ANSWER:", role="assistant"),
            system=(
                "This is a test to verify that you are online. Your response will be validated using a strict "
                "program that does not tolerate any deviations from the expected output at all. Please respond "
                "with these exact words, all capitals and no punctuation: I AM ONLINE"
            ),
            stream=stream,
            max_tokens=20,
            temperature=0,
        )

        result = ""
        async for msg_promise in reply_sequence:
            async for token in msg_promise:
                result += token
            check_response_func(await msg_promise)
    assert result.strip() == "I AM ONLINE"
```



tests/test_message.py
```python
"""
Tests for the `Message`-based models.
"""

import hashlib
import json

import pytest

from miniagents.messages import Message
from miniagents.miniagents import MiniAgents
from miniagents.promising.ext.frozen import Frozen
from miniagents.promising.promising import PromisingContext, Promise
from miniagents.promising.sentinels import DEFAULT


@pytest.mark.asyncio
async def test_message_nesting_vs_hash_key() -> None:
    """
    Test that the hash key of a message is calculated correctly when it contains nested messages (nested messages
    should be replaced with their respective hash keys when the hash is calculated for the ).
    """

    class SpecialNode(Frozen):
        """
        Needed to check if concrete classes are preserved during copying.
        """

    async with PromisingContext():
        message = Message(
            text="юнікод",
            extra_field=[
                15,
                {
                    "role": "user",
                    "nested_nested": (Message(text="nested_text"), Message(text="nested_text2")),
                    "nested_nested2": [Message(text="nested_text2")],
                },
            ],
            extra_node=SpecialNode(nested_nested=Message(text="nested_text3")),
            nested_message=Message(text="nested_text"),
        )

        expected_structure = {
            "class_": "Message",
            "text": "юнікод",
            "text_template": None,
            "extra_field": (
                15,
                {
                    "class_": "Frozen",
                    "role": "user",
                    "nested_nested__hash_keys": (
                        "47e977f85cff13ea8980cf3d76959caec8a4984a",
                        "91868c8c8398b49deb9a04a73c4ea95bdb2eaa65",
                    ),
                    "nested_nested2__hash_keys": ("91868c8c8398b49deb9a04a73c4ea95bdb2eaa65",),
                },
            ),
            "extra_node": {
                "class_": "SpecialNode",
                "nested_nested__hash_key": "25a897f6457abf51fad6a28d86905918bb610038",
            },
            "nested_message__hash_key": "47e977f85cff13ea8980cf3d76959caec8a4984a",
        }
        assert message.serialize() == expected_structure

        expected_hash_key = hashlib.sha256(
            json.dumps(expected_structure, ensure_ascii=False, sort_keys=True).encode("utf-8")
        ).hexdigest()[:40]
        assert message.hash_key == expected_hash_key


# noinspection PyAsyncCall
@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_on_persist_message_event_called_once(start_asap: bool) -> None:
    """
    Assert that the `on_persist_message` event is called only once if the same Message is resolved multiple times.
    """
    promise_resolved_calls = 0
    persist_message_calls = 0

    async def on_promise_resolved(_, __) -> None:
        nonlocal promise_resolved_calls
        promise_resolved_calls += 1

    async def on_persist_message(_, __) -> None:
        nonlocal persist_message_calls
        persist_message_calls += 1

    some_message = Message()

    async with MiniAgents(
        on_promise_resolved=on_promise_resolved,
        on_persist_message=on_persist_message,
    ):
        Promise(prefill_result=some_message, start_asap=start_asap)
        Promise(prefill_result=some_message, start_asap=start_asap)

    assert promise_resolved_calls == 2  # on_promise_resolved should be called twice regardless
    assert persist_message_calls == 1


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_on_persist_message_event_called_twice(start_asap: bool) -> None:
    """
    Assert that the `on_persist_message` event is called twice if two different Messages are resolved.
    """
    promise_resolved_calls = 0
    persist_message_calls = 0

    async def on_promise_resolved(_, __) -> None:
        nonlocal promise_resolved_calls
        promise_resolved_calls += 1

    async def on_persist_message(_, __) -> None:
        nonlocal persist_message_calls
        persist_message_calls += 1

    message1 = Message()
    message2 = Message()

    async with MiniAgents(
        on_promise_resolved=on_promise_resolved,
        on_persist_message=on_persist_message,
    ):
        Promise(prefill_result=message1, start_asap=start_asap)
        Promise(prefill_result=message2, start_asap=start_asap)

    assert promise_resolved_calls == 2  # on_promise_resolved should be called twice regardless
    assert persist_message_calls == 2


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_on_persist_message_event_not_called(start_asap: bool) -> None:
    """
    Assert that the `on_persist_message` event is not called if the resolved value is not a Message.
    """
    promise_resolved_calls = 0
    persist_message_calls = 0

    async def on_promise_resolved(_, __) -> None:
        nonlocal promise_resolved_calls
        promise_resolved_calls += 1

    async def on_persist_message(_, __) -> None:
        nonlocal persist_message_calls
        persist_message_calls += 1

    not_a_message = Frozen(some_field="not a message")

    async with MiniAgents(
        on_promise_resolved=on_promise_resolved,
        on_persist_message=on_persist_message,
    ):
        Promise(prefill_result=not_a_message, start_asap=start_asap)
        Promise(prefill_result=not_a_message, start_asap=start_asap)

    assert promise_resolved_calls == 2  # on_promise_resolved should be called twice regardless
    assert persist_message_calls == 0
```



tests/test_message_sequence.py
```python
"""
Tests for the `MessageSequence` class.
"""

import pytest

from miniagents.messages import Message
from miniagents.miniagents import MessageSequence
from miniagents.promising.promising import PromisingContext
from miniagents.promising.sentinels import DEFAULT


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_message_sequence(start_asap: bool) -> None:
    """
    Assert that `MessageSequence` "flattens" a hierarchy of messages into a flat sequence.
    """
    async with PromisingContext():
        msg_seq1 = MessageSequence(
            appender_capture_errors=True,
            start_asap=start_asap,
        )
        with msg_seq1.message_appender:
            msg_seq1.message_appender.append("msg1")
            msg_seq1.message_appender.append({"text": "msg2", "some_attr": 2})
            msg_seq1.message_appender.append(Message(text="msg3", another_attr=3))

            msg_seq2 = MessageSequence(
                appender_capture_errors=True,
                start_asap=start_asap,
            )
            with msg_seq2.message_appender:
                msg_seq2.message_appender.append("msg4")

                msg_seq3 = MessageSequence(
                    appender_capture_errors=True,
                    start_asap=start_asap,
                )
                with msg_seq3.message_appender:
                    msg_seq3.message_appender.append("msg5")
                    msg_seq3.message_appender.append(["msg6", "msg7"])
                    msg_seq3.message_appender.append([[Message(text="msg8", another_attr=8)]])

                msg_seq2.message_appender.append(msg_seq3.sequence_promise)
                msg_seq2.message_appender.append("msg9")

            msg_seq1.message_appender.append(msg_seq2.sequence_promise)
            msg_seq1.message_appender.append(Message.promise(text="msg10", yet_another_attr=10))
            # msg_seq1.message_appender.append(ValueError("msg11"))

        message_result = [await msg_promise async for msg_promise in msg_seq1.sequence_promise]
        assert message_result == [
            Message(text="msg1"),
            Message(text="msg2", some_attr=2),
            Message(text="msg3", another_attr=3),
            Message(text="msg4"),
            Message(text="msg5"),
            Message(text="msg6"),
            Message(text="msg7"),
            Message(text="msg8", another_attr=8),
            Message(text="msg9"),
            Message(text="msg10", yet_another_attr=10),
            # ValueError("msg11"),
        ]

        token_result = [token async for msg_promise in msg_seq1.sequence_promise async for token in msg_promise]
        assert token_result == [
            "msg1",
            "msg2",
            "msg3",
            "msg4",
            "msg5",
            "msg6",
            "msg7",
            "msg8",
            "msg9",
            "msg10",
            # "msg11",
        ]


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_message_sequence_error(start_asap: bool) -> None:
    """
    Assert that `MessageSequence` "flattens" a hierarchy of messages into a flat sequence, but raises an error at
    the right place.
    """
    async with PromisingContext(appenders_capture_errors_by_default=True):
        msg_seq1 = MessageSequence(start_asap=start_asap)
        with msg_seq1.message_appender:
            msg_seq1.message_appender.append("msg1")

            msg_seq2 = MessageSequence(start_asap=start_asap)
            with msg_seq2.message_appender:
                msg_seq2.message_appender.append("msg2")

                msg_seq3 = MessageSequence(start_asap=start_asap)
                with msg_seq3.message_appender:
                    msg_seq3.message_appender.append("msg3")
                    # msg_seq3.message_appender.append(ValueError("msg4"))
                    raise ValueError("msg5")

                msg_seq2.message_appender.append(msg_seq3.sequence_promise)
                msg_seq2.message_appender.append("msg6")

            msg_seq1.message_appender.append(msg_seq2.sequence_promise)
            msg_seq1.message_appender.append("msg7")

        message_result = []
        with pytest.raises(ValueError, match="msg5"):
            async for msg_promise in msg_seq1.sequence_promise:
                message_result.append(await msg_promise)

    assert message_result == [
        Message(text="msg1"),
        Message(text="msg2"),
        Message(text="msg3"),
        # ValueError("msg4"),
    ]
```



tests/test_promise.py
```python
"""
Tests for the `StreamedPromise` class.
"""

from typing import AsyncIterator

import pytest

from miniagents.promising.promising import StreamedPromise, StreamAppender, PromisingContext
from miniagents.promising.sentinels import DEFAULT


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_stream_replay_iterator(start_asap: bool) -> None:
    """
    Assert that when a `StreamedPromise` is iterated over multiple times, the `streamer` is only called once.
    """
    streamer_iterations = 0

    async def streamer(_streamed_promise: StreamedPromise) -> AsyncIterator[int]:
        nonlocal streamer_iterations
        for i in range(1, 6):
            streamer_iterations += 1
            yield i

    async def resolver(_streamed_promise: StreamedPromise) -> list[int]:
        return [piece async for piece in _streamed_promise]

    async with PromisingContext():
        streamed_promise = StreamedPromise(
            streamer=streamer,
            resolver=resolver,
            start_asap=start_asap,
        )

        assert [i async for i in streamed_promise] == [1, 2, 3, 4, 5]
        # iterate over the promise again
        assert [i async for i in streamed_promise] == [1, 2, 3, 4, 5]

    # test that the streamer is not called multiple times (only 5 real iterations should happen)
    assert streamer_iterations == 5


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_stream_replay_iterator_exception(start_asap: bool) -> None:
    """
    Assert that when a `StreamedPromise` is iterated over multiple times and an exception is raised in the middle of
    the `streamer` iterations, the exact same sequence of exceptions is replayed.
    """

    with StreamAppender(capture_errors=True) as appender:
        for i in range(1, 6):
            if i == 3:
                raise ValueError("Test error")
            appender.append(i)

    async def resolver(_streamed_promise: StreamedPromise) -> list[int]:
        return [piece async for piece in _streamed_promise]

    async def iterate_over_promise():
        promise_iterator = streamed_promise.__aiter__()

        assert await promise_iterator.__anext__() == 1
        assert await promise_iterator.__anext__() == 2
        with pytest.raises(ValueError):
            await promise_iterator.__anext__()
        with pytest.raises(StopAsyncIteration):
            await promise_iterator.__anext__()
        with pytest.raises(StopAsyncIteration):
            await promise_iterator.__anext__()

    async with PromisingContext():
        streamed_promise = StreamedPromise(
            streamer=appender,
            resolver=resolver,
            start_asap=start_asap,
        )

        await iterate_over_promise()
        # iterate over the stream again
        await iterate_over_promise()


async def _async_streamer_but_not_generator(_):
    return  # not a generator


@pytest.mark.parametrize(
    "broken_streamer",
    [
        lambda _: iter([]),  # non-async streamer
        _async_streamer_but_not_generator,
    ],
)
@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_broken_streamer(broken_streamer, start_asap: bool) -> None:
    """
    Assert that when a `StreamedPromise` tries to iterate over a broken `streamer` it does not hang indefinitely, just
    raises an error and stops the stream.
    """

    async def resolver(_streamed_promise: StreamedPromise) -> list[int]:
        return [piece async for piece in _streamed_promise]

    async def iterate_over_promise():
        promise_iterator = streamed_promise.__aiter__()

        with pytest.raises((TypeError, AttributeError)):
            await promise_iterator.__anext__()
        with pytest.raises(StopAsyncIteration):
            await promise_iterator.__anext__()
        with pytest.raises(StopAsyncIteration):
            await promise_iterator.__anext__()

    async with PromisingContext():
        streamed_promise = StreamedPromise(
            streamer=broken_streamer,
            resolver=resolver,
            start_asap=start_asap,
        )

        await iterate_over_promise()
        # iterate over the stream again
        await iterate_over_promise()


@pytest.mark.parametrize(
    "broken_resolver",
    [
        lambda _: [],  # non-async resolver
        TypeError,
    ],
)
@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_broken_stream_resolver(broken_resolver, start_asap: bool) -> None:
    """
    Assert that if `resolver` is broken, `StreamedPromise` still yields the stream and only fails upon `aresolve()`
    (or bare `await`, for that matter).
    """
    expected_resolver_call_count = 0  # we are not counting resolver calls for completely broken resolvers (too hard)
    actual_resolver_call_count = 0
    if isinstance(broken_resolver, type):
        expected_resolver_call_count = 1  # we are counting resolver calls for the partially broken resolver
        error_class = broken_resolver

        async def broken_resolver(_streamed_promise: StreamedPromise) -> None:  # pylint: disable=function-redefined
            nonlocal actual_resolver_call_count
            actual_resolver_call_count += 1
            raise error_class("Test error")

    with StreamAppender(capture_errors=True) as appender:
        for i in range(1, 6):
            appender.append(i)

    async with PromisingContext():
        streamed_promise = StreamedPromise(
            streamer=appender,
            resolver=broken_resolver,
            start_asap=start_asap,
        )

        with pytest.raises(TypeError) as exc_info1:
            await streamed_promise
        error1 = exc_info1.value

        assert [i async for i in streamed_promise] == [1, 2, 3, 4, 5]

        with pytest.raises(TypeError) as exc_info2:
            await streamed_promise

    assert error1 is exc_info2.value  # exact same error instance should be raised again

    assert actual_resolver_call_count == expected_resolver_call_count


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_streamed_promise_aresolve(start_asap: bool) -> None:
    """
    Assert that:
    - when a `StreamedPromise` is "resolved" multiple times, the `resolver` is only called once;
    - the exact same instance of the result object is returned from `aresolve()` when it is called again.
    """
    resolver_calls = 0

    with StreamAppender(capture_errors=False) as appender:
        for i in range(1, 6):
            appender.append(i)

    async def resolver(_streamed_promise: StreamedPromise) -> list[int]:
        nonlocal resolver_calls
        resolver_calls += 1
        return [piece async for piece in _streamed_promise]

    async with PromisingContext():
        streamed_promise = StreamedPromise(
            streamer=appender,
            resolver=resolver,
            start_asap=start_asap,
        )

        result1 = await streamed_promise
        # "collect from the stream" again
        result2 = await streamed_promise

        # test that the resolver is not called multiple times
        assert resolver_calls == 1

        assert result1 == [1, 2, 3, 4, 5]
        assert result2 is result1  # the promise should always return the exact same instance of the result object


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_stream_appender_dont_capture_errors(start_asap: bool) -> None:
    """
    Assert that when `StreamAppender` is not capturing errors, then:
    - the error is raised beyond the context manager;
    - the `StreamedPromise` is not affected by the error and is just returning the elements up to the error.
    """
    with pytest.raises(ValueError):
        with StreamAppender(capture_errors=False) as appender:
            for i in range(1, 6):
                if i == 3:
                    raise ValueError("Test error")
                appender.append(i)

    async def resolver(_streamed_promise: StreamedPromise) -> list[int]:
        return [piece async for piece in _streamed_promise]

    async with PromisingContext():
        streamed_promise = StreamedPromise(
            streamer=appender,
            resolver=resolver,
            start_asap=start_asap,
        )

        assert await streamed_promise == [1, 2]


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_streamed_promise_same_instance(start_asap: bool) -> None:
    """
    Assert that `streamer` and `resolver` receive the exact same instance of `StreamedPromise`.
    """

    async def streamer(_streamed_promise: StreamedPromise) -> AsyncIterator[int]:
        assert _streamed_promise is streamed_promise
        yield 1

    async def resolver(_streamed_promise: StreamedPromise) -> list[int]:
        assert _streamed_promise is streamed_promise
        return [piece async for piece in _streamed_promise]

    async with PromisingContext():
        streamed_promise = StreamedPromise(
            streamer=streamer,
            resolver=resolver,
            start_asap=start_asap,
        )

        await streamed_promise
```



tests/test_sequence.py
```python
"""
Tests for the `FlatSequence` class.
"""

from typing import AsyncIterator

import pytest

from miniagents.promising.promising import PromisingContext, StreamAppender
from miniagents.promising.sentinels import DEFAULT
from miniagents.promising.sequence import FlatSequence


@pytest.mark.parametrize("start_asap", [False, True, DEFAULT])
@pytest.mark.asyncio
async def test_flat_sequence(start_asap: bool) -> None:
    """
    Assert that `FlatSequence` "flattens" the input sequence of (0, 1, 2, 3) into the output sequence of
    (1, 2, 2, 3, 3, 3), in accordance with the flattener function that is passed to its constructor.
    """

    async def flattener(_, number: int) -> AsyncIterator[int]:
        for _ in range(number):
            yield number

    async with PromisingContext():
        stream_appender = StreamAppender[int](capture_errors=True)
        flat_sequence = FlatSequence[int, int](
            incoming_streamer=stream_appender,
            flattener=flattener,
            start_asap=start_asap,
        )
        with stream_appender:
            stream_appender.append(0)
            stream_appender.append(1)
            stream_appender.append(2)
            stream_appender.append(3)

        assert await flat_sequence.sequence_promise == (1, 2, 2, 3, 3, 3)
        assert [i async for i in flat_sequence.sequence_promise] == [1, 2, 2, 3, 3, 3]
```